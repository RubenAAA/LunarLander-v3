{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ”‡ Suppressing non-critical warnings for a cleaner output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "# Ignore specific warnings about PPO running on GPU (it's CPU-optimized)\n",
    "warnings.filterwarnings(\"ignore\", message=\"You are trying to run PPO on the GPU, but it is primarily intended to run on the CPU\")\n",
    "\n",
    "# Disable TensorFlow TensorRT GPU fallback (not needed for this implementation)\n",
    "os.environ[\"TF_TRT_ALLOW_GPU_FALLBACK\"] = \"0\"\n",
    "\n",
    "# Suppress TensorFlow debug logs\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "logging.getLogger('tensorflow').setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“Œ Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Standard libraries \n",
    "import time\n",
    "import json\n",
    "import random\n",
    "import multiprocessing\n",
    "\n",
    "## Numerical & deep learning libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "\n",
    "## Reinforcement Learning & Gym\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.callbacks import BaseCallback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ² Setting Random Seed for Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1337  # Fixed seed value for consistent results\n",
    "\n",
    "# Set seed for Python's built-in random module\n",
    "random.seed(SEED)\n",
    "\n",
    "# Set seed for NumPy (affects all NumPy-based random operations)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Set seed for PyTorch (affects all tensor operations using randomization)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# Set seed for CUDA operations (ensures deterministic behavior on GPUs)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# Ensure deterministic computations in cuDNN (may affect performance slightly)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âš¡ Optimizing PyTorch Performance on CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable cuDNN auto-tuner to find the best algorithms for the current hardware\n",
    "torch.backends.cudnn.benchmark = True  \n",
    "\n",
    "# Ensure cuDNN is enabled (improves performance for convolutional operations)\n",
    "torch.backends.cudnn.enabled = True  \n",
    "\n",
    "# Set higher precision for float32 matrix multiplications (boosts performance)\n",
    "torch.set_float32_matmul_precision(\"high\")  \n",
    "\n",
    "# Free up unused GPU memory to avoid fragmentation issues\n",
    "torch.cuda.empty_cache()  \n",
    "\n",
    "# Allocate up to 95% of GPU memory for this process (prevents memory overflows)\n",
    "torch.cuda.set_per_process_memory_fraction(0.95)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ› ï¸ Setting Multiprocessing Start Method. (Required by CUDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the \"spawn\" method for starting new processes.\n",
    "# This is recommended for PyTorch to avoid issues with shared memory.\n",
    "# BUT, doesn't work in Jupyter, so no point defining it\n",
    "\n",
    "# multiprocessing.set_start_method(\"spawn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model save path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = \"final\"  # Global model save path\n",
    "\n",
    "# Alternatively, select the best one I trained\n",
    "# MODEL_PATH = \"final_296\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Adaptive Hyperparameter Tuning for RL\n",
    "\n",
    "This callback is the **core differentiator** of the modelâ€”it introduces a **dynamic hyperparameter adjustment mechanism** that reacts to training performance in real-time. \n",
    "\n",
    "Unlike static training setups, it **modifies exploration, optimization, and learning rate on the fly**, based on episode length and rewards.\n",
    "\n",
    "### Key Features:\n",
    "- **ðŸš€ Adapts to training conditions**:\n",
    "  - If training **stagnates with long episodes**, it **increases exploration**.\n",
    "  - If **rewards improve**, it **fine-tunes learning stability**.\n",
    "  - If **performance drops**, it **resets to baseline** for recovery.\n",
    "\n",
    "This **adaptive tuning** allows the model to **self-optimize over time**, making learning more **efficient and responsive** to training conditions. ðŸš€\n",
    "\n",
    "-------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdjustHyperparamsCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    Hyperparameters managed:\n",
    "    - `ent_coef` (Entropy coefficient): Controls exploration.\n",
    "    - `vf_coef` (Value function coefficient): Balances policy and value loss.\n",
    "    - `n_epochs` (Number of training epochs): Defines how often each batch is trained.\n",
    "    - `lr` (Learning rate): Adjusted dynamically for better convergence.\n",
    "    - `threshold` (Episode length threshold): Used to decide when to modify hyperparameters.\n",
    "\n",
    "    The updated hyperparameters are saved to a JSON file (`hyperparams.json`) to maintain consistency across training runs.\n",
    "\n",
    "    Attributes:\n",
    "        env (VecEnv): The training environment (vectorized).\n",
    "        percentage (float): Fraction of max episode steps used as threshold for modification.\n",
    "        window_size (int): Number of recent episodes to consider for statistics.\n",
    "        save_path (str): Path to save hyperparameters.\n",
    "        modified_length (bool): Tracks if episode length-based modifications were applied.\n",
    "        modified_reward (bool): Tracks if reward-based modifications were applied.\n",
    "        ent_coef (float): Entropy coefficient for exploration.\n",
    "        vf_coef (float): Value function coefficient.\n",
    "        n_epochs (int): Number of training epochs.\n",
    "        lr (float): Learning rate.\n",
    "\n",
    "    Methods:\n",
    "        save_hyperparams():\n",
    "            Saves the updated hyperparameters to a JSON file.\n",
    "\n",
    "        load_hyperparams():\n",
    "            Loads hyperparameters from a JSON file if it exists.\n",
    "\n",
    "        _on_step():\n",
    "            Evaluates recent training performance and adjusts hyperparameters dynamically.\n",
    "    \"\"\"\n",
    "    def __init__(self, env, percentage=1, window_size=100, verbose=0, save_path=\"hyperparams.json\"):\n",
    "        super().__init__(verbose)\n",
    "        self.window_size = window_size  # Moving average window\n",
    "        self.save_path = save_path\n",
    "\n",
    "        # Compute a global threshold based on the average max_episode_steps across envs\n",
    "        max_steps = np.mean([e.spec.max_episode_steps for e in env.envs])\n",
    "        self.threshold = int(max_steps * percentage)\n",
    "\n",
    "        # Global flags to track if modifications have been applied\n",
    "        self.modified_length = False\n",
    "        self.modified_reward = False\n",
    "\n",
    "        # Default global hyperparameters\n",
    "        self.ent_coef = 0.01\n",
    "        # self.gamma = 0.999\n",
    "        self.vf_coef = 0.5\n",
    "        self.n_epochs = 8\n",
    "        self.lr = 0.01\n",
    "\n",
    "        # Load previous hyperparameters if available\n",
    "        self.load_hyperparams()\n",
    "\n",
    "    def save_hyperparams(self):\n",
    "        \"\"\"Save modified hyperparameters to a JSON file.\"\"\"\n",
    "        hyperparams = {\n",
    "            \"ent_coef\": self.ent_coef,\n",
    "            # \"gamma\": self.gamma,\n",
    "            \"vf_coef\": self.vf_coef,\n",
    "            \"n_epochs\": self.n_epochs,\n",
    "            \"modified_length\": self.modified_length,\n",
    "            \"modified_reward\": self.modified_reward,\n",
    "            \"threshold\": self.threshold,\n",
    "            \"lr\": self.lr\n",
    "        }\n",
    "        with open(self.save_path, \"w\") as f:\n",
    "            json.dump(hyperparams, f)\n",
    "    \n",
    "    def load_hyperparams(self):\n",
    "        \"\"\"Load modified hyperparameters from a JSON file if it exists.\"\"\"\n",
    "        if os.path.exists(self.save_path):\n",
    "            with open(self.save_path, \"r\") as f:\n",
    "                hyperparams = json.load(f)\n",
    "                self.ent_coef = hyperparams.get(\"ent_coef\", self.ent_coef)\n",
    "                # self.gamma = hyperparams.get(\"gamma\", self.gamma)\n",
    "                self.vf_coef = hyperparams.get(\"vf_coef\", self.vf_coef)\n",
    "                self.n_epochs = hyperparams.get(\"n_epochs\", self.n_epochs)\n",
    "                self.modified_length = hyperparams.get(\"modified_length\", self.modified_length)\n",
    "                self.modified_reward = hyperparams.get(\"modified_reward\", self.modified_reward)\n",
    "                self.threshold = hyperparams.get(\"threshold\", self.threshold)\n",
    "                self.lr = hyperparams.get(\"lr\", self.lr)\n",
    "                print(\"âœ… Loaded previous hyperparameters from file.\")\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        \"\"\"\n",
    "        Dynamically adjusts hyperparameters based on recent training performance.\n",
    "\n",
    "        This method is executed at each training step and analyzes recent episode statistics (length and reward)\n",
    "        to modify key hyperparameters adaptively. The goal is to:\n",
    "        1. Increase exploration if episodes are too long but rewards are high.\n",
    "        2. Fine-tune learning rate and optimization parameters based on reward trends.\n",
    "        3. Reset hyperparameters if performance drops.\n",
    "\n",
    "        Hyperparameters adjusted:\n",
    "        - `ent_coef`: Entropy coefficient (controls exploration).\n",
    "        - `vf_coef`: Value function coefficient.\n",
    "        - `n_epochs`: Number of training epochs.\n",
    "        - `lr`: Learning rate.\n",
    "        - `threshold`: Adjusted based on training performance.\n",
    "\n",
    "        Returns:\n",
    "            bool: `True` to continue training, `False` to stop the current `.learn()` call if major adjustments occur.\n",
    "        \"\"\"\n",
    "\n",
    "        lr_min, lr_max = 0.0001, 0.01  # Define learning rate range\n",
    "\n",
    "        # Ensure there is at least one episode in the buffer.\n",
    "        if self.model.ep_info_buffer:\n",
    "\n",
    "            # Compute global statistics over the most recent episodes.\n",
    "            recent_eps = list(self.model.ep_info_buffer)[-self.window_size:]\n",
    "            recent_ep_lengths = [ep[\"l\"] for ep in recent_eps]\n",
    "            recent_ep_rewards = [ep[\"r\"] for ep in recent_eps]\n",
    "\n",
    "            smoothed_ep_len_mean = np.mean(recent_ep_lengths)\n",
    "            smoothed_ep_rew_mean = np.mean(recent_ep_rewards)\n",
    "\n",
    "            # ------------------------------------------------------------\n",
    "            # 1. If the episode length is too high and rewards are high, \n",
    "            #    increase exploration and adjust learning rate.\n",
    "            # ------------------------------------------------------------\n",
    "            if smoothed_ep_len_mean >= self.threshold and smoothed_ep_rew_mean > 120 and not self.modified_length:\n",
    "                self.ent_coef = 0.015  # Increase entropy to encourage exploration\n",
    "                # self.gamma = 0.99  # Focus on short-term rewards\n",
    "                \n",
    "                # Adjust learning rate based on reward scaling\n",
    "                self.lr = lr_min + (lr_max - lr_min) * np.log1p((315 - smoothed_ep_rew_mean) / 315) / np.log1p(1)\n",
    "\n",
    "                self.modified_length = True  # Mark that modifications were applied\n",
    "\n",
    "                if self.verbose > 0:\n",
    "                    print(f\"ðŸ”„ High episode length (mean: {smoothed_ep_len_mean:.2f}) detected.\")\n",
    "                    print(f\"    Updating hyperparams: ent_coef={self.ent_coef}, gamma={self.gamma}\")\n",
    "\n",
    "                self.save_hyperparams()\n",
    "                return False  # Stop training for adjustments to take effect\n",
    "\n",
    "            # ------------------------------------------------------------\n",
    "            # 2. If episode length is below the threshold and adjustments \n",
    "            #    were previously made, fine-tune based on reward trends.\n",
    "            # ------------------------------------------------------------\n",
    "            elif smoothed_ep_len_mean < self.threshold and self.modified_length:\n",
    "                self.lr = lr_min + (lr_max - lr_min) * np.log1p((315 - smoothed_ep_rew_mean) / 315) / np.log1p(1)\n",
    "\n",
    "                # If rewards are high, refine hyperparameters to improve performance\n",
    "                if smoothed_ep_rew_mean >= 200:\n",
    "                    min_ent_coef, max_ent_coef = 0.001, 0.01\n",
    "                    # min_gamma, max_gamma = 0.999, 0.9999\n",
    "                    min_vf_coef, max_vf_coef = 0.5, 1.0\n",
    "                    min_n_epochs, max_n_epochs = 8, 28\n",
    "\n",
    "                    # Normalize reward between 0 and 1\n",
    "                    normalized_reward = max(0, min((smoothed_ep_rew_mean - 200) / (315 - 200), 1))\n",
    "                    self.ent_coef = max_ent_coef - (max_ent_coef - min_ent_coef) * normalized_reward\n",
    "                    # self.gamma = min_gamma + (max_gamma - min_gamma) * normalized_reward\n",
    "                    self.vf_coef = min_vf_coef + (max_vf_coef - min_vf_coef) * normalized_reward\n",
    "                    self.n_epochs = int(min_n_epochs + (max_n_epochs - min_n_epochs) * normalized_reward)\n",
    "\n",
    "                    # Gradually decrease the threshold\n",
    "                    self.threshold = max(self.threshold - int(3 * normalized_reward), 130)\n",
    "\n",
    "                    self.modified_reward = True  # Mark that reward-based modifications were applied\n",
    "\n",
    "                    if self.verbose > 0:\n",
    "                        print(f\"ðŸ”„ Adjusted hyperparams based on high reward (mean: {smoothed_ep_rew_mean:.2f})\")\n",
    "                        # print(f\"    ent_coef: {self.ent_coef:.5f}, gamma: {self.gamma:.6f}, vf_coef: {self.vf_coef:.2f}\")\n",
    "\n",
    "                # ------------------------------------------------------------\n",
    "                # 3. If rewards drop significantly, reset hyperparameters.\n",
    "                # ------------------------------------------------------------\n",
    "                elif smoothed_ep_rew_mean < 175 and self.modified_reward:\n",
    "                    self.ent_coef = 0.01  # Reset entropy coefficient\n",
    "                    # self.gamma = 0.999  # Reset gamma\n",
    "                    self.vf_coef = 0.5  # Reset value function coefficient\n",
    "                    self.n_epochs = 8  # Reset epochs to default\n",
    "                    self.modified_reward = False  # Reset flag\n",
    "\n",
    "                    if self.verbose > 0:\n",
    "                        print(\"ðŸ”„ Reset hyperparameters due to reward drop.\")\n",
    "\n",
    "            # ------------------------------------------------------------\n",
    "            # Apply the final hyperparameter changes to the model.\n",
    "            # ------------------------------------------------------------\n",
    "            self.model.ent_coef = self.ent_coef\n",
    "            # self.model.gamma = self.gamma\n",
    "            self.model.vf_coef = self.vf_coef\n",
    "            self.model.n_epochs = self.n_epochs\n",
    "\n",
    "            # Save updated hyperparameters to JSON\n",
    "            self.save_hyperparams()\n",
    "\n",
    "        return True  # Continue training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation Function\n",
    "\n",
    "Next, we define the `evaluate` function, which will **run the evaluation process** for a trained reinforcement learning model in the **LunarLander-v3** environment. \n",
    "\n",
    "This function serves three main purposes:\n",
    "\n",
    "1. **Scoring the Model**  \n",
    "   - It runs multiple episodes and calculates the **total reward** obtained by the agent in each episode.  \n",
    "   - It also records the **number of steps** the agent takes before reaching a terminal state (either success or failure).  \n",
    "   - These metrics help assess how well the model performs in terms of both efficiency and effectiveness.\n",
    "\n",
    "2. **Providing a Visualization**  \n",
    "   - The environment is launched with `render_mode=\"human\"`, allowing real-time visualization of the agentâ€™s actions.  \n",
    "   - The function uses `env.render()` at each step to display the agent's interactions with the environment.  \n",
    "   - A **small delay (`time.sleep(0.01)`)** is introduced to make the animation smoother and easier to follow.\n",
    "\n",
    "3. **Ensuring Controlled Execution**  \n",
    "   - The model is loaded from a specified path and evaluated using deterministic policy execution (`deterministic=True`), ensuring repeatability.  \n",
    "   - The function limits the number of steps per episode to **250** to prevent showing landing episodes that last too long, potentially causing multiple eval visualizations to run at the same time.  \n",
    "   - After evaluation, the environment is properly closed using `env.close()` to free system resources.\n",
    "\n",
    "This evaluation step is crucial for diagnosing the modelâ€™s performance and identifying areas for improvement, such as hyperparameter tuning or additional training.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model_path, episodes=3):\n",
    "    \"\"\"\n",
    "    Evaluates a trained PPO model on the LunarLander-v3 environment.\n",
    "\n",
    "    This function loads a pretrained model, runs it in the environment for a specified \n",
    "    number of episodes, and renders the gameplay while tracking the total reward and steps.\n",
    "\n",
    "    Features:\n",
    "    - Uses `render_mode=\"human\"` to visualize the agent's behavior.\n",
    "    - Loads the model from the specified `model_path` and selects GPU (`cuda`) if available.\n",
    "    - Runs multiple episodes, tracking rewards and number of steps per episode.\n",
    "    - Enforces a maximum step limit of 250 per episode for stability.\n",
    "    - Introduces a small time delay (`0.01s`) between steps to make visualization smoother.\n",
    "\n",
    "    Args:\n",
    "        model_path (str): Path to the saved PPO model file.\n",
    "        episodes (int): Number of episodes to evaluate. Default is 3.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create the environment with rendering enabled for visualization.\n",
    "    env = gym.make(\"LunarLander-v3\", render_mode=\"human\")\n",
    "\n",
    "    # Load the trained PPO model with the environment attached.\n",
    "    model = PPO.load(model_path, env=env, device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    for ep in range(episodes):\n",
    "        # Reset the environment for a new episode.\n",
    "        obs, info = env.reset()\n",
    "        done = False\n",
    "        truncated = False\n",
    "        ep_reward = 0  # Track total episode reward\n",
    "        steps = 0  # Track number of steps in the episode\n",
    "\n",
    "        while not (done or truncated):\n",
    "            env.render()  # Render the environment for visualization\n",
    "            \n",
    "            # Select action from the trained model (deterministic mode for evaluation)\n",
    "            action, _ = model.predict(obs, deterministic=True)\n",
    "            \n",
    "            # Execute the action in the environment and receive the next state and reward.\n",
    "            obs, reward, done, truncated, info = env.step(action)\n",
    "            \n",
    "            ep_reward += reward  # Accumulate total reward\n",
    "            steps += 1  # Count steps\n",
    "\n",
    "            # Enforce a maximum step limit per episode to prevent infinite loops\n",
    "            if steps == 250:\n",
    "                done = True\n",
    "\n",
    "            time.sleep(0.01)  # Small delay for smoother rendering\n",
    "\n",
    "        print(f\"Episode {ep+1} reward: {ep_reward:.4f}, Steps= {steps}\")\n",
    "\n",
    "    env.close()  # Close the environment to free resources\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸš€ Training PPO on LunarLander-v3 with Adaptive Hyperparameters\n",
    "\n",
    "This function trains a **Proximal Policy Optimization (PPO) agent** in the **LunarLander-v3** environment, leveraging **dynamic hyperparameter tuning** to optimize performance over time.\n",
    "\n",
    "### ðŸ”¹ Key Features:\n",
    "- **Automatic Hyperparameter Adjustment**  \n",
    "  - Uses `AdjustHyperparamsCallback` to modify **exploration**, **learning rate**, and other key parameters in response to training conditions.\n",
    "  \n",
    "- **Parallel Training for Faster Convergence**  \n",
    "  - Runs **64 parallel environments** to accelerate learning.\n",
    "\n",
    "- **Model Persistence & Periodic Checkpoints**  \n",
    "  - Automatically **loads an existing model** if available, allowing incremental training.\n",
    "  - Periodically **saves the model** and **evaluates it in a separate process**.\n",
    "\n",
    "- **Restart Mechanism for Adaptive Learning**  \n",
    "  - If performance stagnates or improves significantly, the PPO model **restarts with new hyperparameters**.\n",
    "\n",
    "### ðŸ“Œ Training Details:\n",
    "- **Policy Architecture:** 2-layer MLP with sizes `[128, 64]`\n",
    "- **Optimizer:** `AdamW` with weight decay (`1e-2`)\n",
    "- **Training Steps:** `user defined`\n",
    "- **Evaluation Frequency:** Every `500,000` steps\n",
    "\n",
    "This implementation ensures that the agent **adapts dynamically**, making it more **robust and efficient** compared to static training approaches. ðŸš€\n",
    "\n",
    "\n",
    "--------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_ppo(total_timesteps):\n",
    "    \"\"\"\n",
    "    Train a PPO (Proximal Policy Optimization) agent on the LunarLander-v3 environment.\n",
    "\n",
    "    This function initializes or loads a PPO model and continuously trains it while adjusting \n",
    "    hyperparameters dynamically. The model is periodically restarted when the `AdjustHyperparamsCallback` \n",
    "    signals that adjustments are necessary.\n",
    "\n",
    "    Key Features:\n",
    "    - Uses **Stable-Baselines3 PPO** with a custom neural network architecture.\n",
    "    - Supports **dynamic hyperparameter tuning** via the `AdjustHyperparamsCallback`.\n",
    "    - Implements **periodic model saving** and evaluation in a separate process.\n",
    "    - Trains on **multiple environments in parallel (64 environments)** for faster learning.\n",
    "    - Uses **adaptive exploration strategies** to optimize agent performance.\n",
    "\n",
    "    Args:\n",
    "        total_timesteps (int): The total number of training timesteps before stopping.\n",
    "    \"\"\"\n",
    "\n",
    "    # Number of parallel environments to use for training\n",
    "    n_envs = 64  \n",
    "\n",
    "    # Flag to track if reward-based modifications have been applied\n",
    "    rew_mod_anytime = False  \n",
    "\n",
    "    # Define the policy architecture and optimizer settings\n",
    "    policy_kwargs = dict(\n",
    "        optimizer_class=AdamW,  # Use AdamW optimizer for better weight decay handling\n",
    "        optimizer_kwargs=dict(weight_decay=1e-2),  # Weight decay for regularization\n",
    "        net_arch=dict(pi=[128, 64], vf=[128, 64])  # Define network structure for policy and value functions\n",
    "    )\n",
    "\n",
    "    # Create a vectorized environment with a fixed episode length of 250 steps\n",
    "    env = make_vec_env(\"LunarLander-v3\", n_envs=n_envs, env_kwargs={\"max_episode_steps\": 250})\n",
    "\n",
    "    # Set a fixed random seed for reproducibility\n",
    "    env.seed(SEED)\n",
    "\n",
    "    # Select the computation device: Use GPU if available, otherwise fall back to CPU\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    # -----------------------------------------------\n",
    "    # Load an existing model if available; otherwise, train from scratch\n",
    "    # -----------------------------------------------\n",
    "    if os.path.exists(MODEL_PATH + \".zip\"):\n",
    "        print(f\"Loading existing model from {MODEL_PATH}...\")\n",
    "\n",
    "        # Load hyperparameters from a JSON file (if previously saved)\n",
    "        with open(\"hyperparams.json\", \"r\") as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        lr_value = data.get(\"lr\")  # Extract the stored learning rate\n",
    "\n",
    "        # Load the pretrained PPO model with the environment and existing hyperparameters\n",
    "        model = PPO.load(\n",
    "            MODEL_PATH,\n",
    "            env=env,\n",
    "            device=device,\n",
    "            learning_rate=lr_value,  # Attempt to apply the saved learning rate\n",
    "            custom_objects=data,  # Load other hyperparameters dynamically\n",
    "            policy_kwargs=policy_kwargs\n",
    "        )\n",
    "\n",
    "        # Manually update the optimizer's learning rate (alternative approach)\n",
    "        for param_group in model.policy.optimizer.param_groups:\n",
    "            param_group[\"lr\"] = lr_value\n",
    "\n",
    "        # Get the current number of timesteps already trained\n",
    "        current_timesteps = model.num_timesteps\n",
    "\n",
    "    else:\n",
    "        print(\"No existing model found. Training from scratch...\")\n",
    "\n",
    "        # Initialize a new PPO model with predefined hyperparameters\n",
    "        model = PPO(\n",
    "            policy=\"MlpPolicy\",  # Use a Multi-Layer Perceptron (MLP) policy\n",
    "            env=env,\n",
    "            learning_rate=lambda _: 0.01,  # Initial learning rate (adjustable later)\n",
    "            n_steps=512,  # Number of steps per environment before updating\n",
    "            batch_size=4096,  # Batch size for training updates\n",
    "            n_epochs=8,  # Number of passes over each batch during training\n",
    "            gamma=0.999,  # Discount factor for future rewards\n",
    "            gae_lambda=0.98,  # Generalized Advantage Estimation lambda\n",
    "            clip_range=lambda _: 0.2,  # Clipping range for PPO objective\n",
    "            ent_coef=0.01,  # Entropy coefficient (controls exploration)\n",
    "            vf_coef=0.5,  # Value function coefficient (importance of critic loss)\n",
    "            max_grad_norm=0.5,  # Gradient clipping for stability\n",
    "            use_sde=False,  # Whether to use State-Dependent Exploration (SDE)\n",
    "            normalize_advantage=True,  # Normalize advantages for stability\n",
    "            verbose=1,  # Print training logs\n",
    "            device=device,  # Set computation device (GPU/CPU)\n",
    "            tensorboard_log=\"./final_tensorboard\",  # Path for TensorBoard logs\n",
    "            policy_kwargs=policy_kwargs  # Apply custom network architecture\n",
    "        )\n",
    "\n",
    "        # Since training starts fresh, initialize current timestep count to 0\n",
    "        current_timesteps = 0\n",
    "\n",
    "    # -----------------------------------------------\n",
    "    # Initialize the hyperparameter tuning callback\n",
    "    # -----------------------------------------------\n",
    "    callback = AdjustHyperparamsCallback(\n",
    "        env, percentage=0.999, window_size=n_envs * 512 * 2, verbose=0  # You can turn verbosity on but it will cause a lot of outputs\n",
    "    )\n",
    "\n",
    "    # -----------------------------------------------\n",
    "    # Main training loop with periodic evaluation\n",
    "    # -----------------------------------------------\n",
    "    eval_interval = 500_000  # Evaluate the model every 500,000 steps\n",
    "\n",
    "    while current_timesteps < total_timesteps:\n",
    "        # Determine how many steps to train in this cycle\n",
    "        train_step = min(eval_interval, total_timesteps - current_timesteps)\n",
    "\n",
    "        # Retrieve updated hyperparameters from the callback\n",
    "        new_hyperparams = {\n",
    "            \"ent_coef\": callback.ent_coef,\n",
    "            # \"gamma\": callback.gamma,\n",
    "            \"vf_coef\": callback.vf_coef,\n",
    "            \"n_epochs\": callback.n_epochs,\n",
    "            \"threshold\": callback.threshold,\n",
    "            \"modified_length\": callback.modified_length,\n",
    "            \"modified_reward\": callback.modified_reward,\n",
    "            \"lr\": callback.lr,      \n",
    "        }\n",
    "\n",
    "        # Restart training with updated hyperparameters if needed\n",
    "        if os.path.exists(MODEL_PATH + \".zip\"):\n",
    "            # Track whether rewards have been modified anytime during training\n",
    "            rew_mod_anytime = rew_mod_anytime or callback.modified_reward\n",
    "\n",
    "            print(f\"Restarting PPO with new hyperparameters: {new_hyperparams}\")\n",
    "\n",
    "            if rew_mod_anytime:\n",
    "                # Reload the model with the new hyperparameters if reward-based modifications were applied\n",
    "                model = PPO.load(\n",
    "                    MODEL_PATH,\n",
    "                    env=env,\n",
    "                    device=device,\n",
    "                    learning_rate=callback.lr,  # Apply the new learning rate\n",
    "                    custom_objects=new_hyperparams,  # Load updated hyperparams\n",
    "                    policy_kwargs=policy_kwargs\n",
    "                )\n",
    "                for param_group in model.policy.optimizer.param_groups:\n",
    "                    param_group[\"lr\"] = callback.lr\n",
    "            else:\n",
    "                # Reload the model with non-reward-modified hyperparameters\n",
    "                model = PPO.load(\n",
    "                    MODEL_PATH,\n",
    "                    env=env,\n",
    "                    device=device,\n",
    "                    custom_objects=new_hyperparams,\n",
    "                    policy_kwargs=policy_kwargs\n",
    "                )\n",
    "\n",
    "        # -----------------------------------------------\n",
    "        # Train the model for the current step interval\n",
    "        # -----------------------------------------------\n",
    "        model.learn(total_timesteps=train_step, reset_num_timesteps=False, callback=callback)\n",
    "        current_timesteps += train_step  # Update the total timesteps count\n",
    "\n",
    "        # -----------------------------------------------\n",
    "        # Save the model after training iteration\n",
    "        # -----------------------------------------------\n",
    "        model._logger = None  # Remove logger to avoid pickling issues\n",
    "        model.save(MODEL_PATH)  # Save the trained model\n",
    "\n",
    "        # -----------------------------------------------\n",
    "        # Launch evaluation in a separate process\n",
    "        # This doesn't work in the jupyter environment so we comment it out here\n",
    "        # -----------------------------------------------\n",
    "        # eval_process = multiprocessing.Process(target=evaluate, args=(MODEL_PATH,))\n",
    "        # eval_process.start()  # Run evaluation asynchronously\n",
    "\n",
    "    # Close the environment after training is complete\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Running PPO Training\n",
    "\n",
    "Now that we have defined our **adaptive PPO training function**, it's time to **start training** the agent on **LunarLander-v3**.\n",
    "\n",
    "### ðŸ”¥ Training Execution:\n",
    "- The model will train for **20,000,000 timesteps**.\n",
    "- If an **existing model** is found, training will **resume** from the last checkpoint.\n",
    "- Adaptive hyperparameter tuning is enabled via `AdjustHyperparamsCallback`.\n",
    "- The model will be periodically **evaluated and saved** to prevent loss of progress.\n",
    "\n",
    "â³ Expected Duration:\n",
    "Training for 20M timesteps may take a significant amount of time depending on:\n",
    "\n",
    "- GPU availability (CUDA acceleration).\n",
    "- Number of CPU cores used for parallel environments.\n",
    "- Training efficiency (batch sizes, optimization settings).\n",
    "\n",
    "\n",
    "Monitor training logs for progress and periodic evaluations! ðŸš€\n",
    "\n",
    "-------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with PPO...\n",
      "Loading existing model from final...\n",
      "âœ… Loaded previous hyperparameters from file.\n",
      "Restarting PPO with new hyperparameters: {'ent_coef': 0.003113711996695648, 'vf_coef': 0.8825715557391307, 'n_epochs': 23, 'threshold': 161, 'modified_length': True, 'modified_reward': True, 'lr': 0.0012749372513232453}\n",
      "Logging to ./final_tensorboard/PPO_0\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 168      |\n",
      "|    ep_rew_mean     | 287      |\n",
      "| time/              |          |\n",
      "|    fps             | 2720     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 12       |\n",
      "|    total_timesteps | 10223616 |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 170          |\n",
      "|    ep_rew_mean          | 291          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2684         |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 24           |\n",
      "|    total_timesteps      | 10256384     |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037401249 |\n",
      "|    clip_fraction        | 0.0311       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.4         |\n",
      "|    explained_variance   | 0.981        |\n",
      "|    learning_rate        | 0.00127      |\n",
      "|    loss                 | 50.4         |\n",
      "|    n_updates            | 6220         |\n",
      "|    policy_gradient_loss | -0.00206     |\n",
      "|    value_loss           | 69.1         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 168          |\n",
      "|    ep_rew_mean          | 293          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2348         |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 41           |\n",
      "|    total_timesteps      | 10289152     |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034742234 |\n",
      "|    clip_fraction        | 0.0294       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.392       |\n",
      "|    explained_variance   | 0.993        |\n",
      "|    learning_rate        | 0.00127      |\n",
      "|    loss                 | 9.44         |\n",
      "|    n_updates            | 6243         |\n",
      "|    policy_gradient_loss | -0.00152     |\n",
      "|    value_loss           | 21.6         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 169          |\n",
      "|    ep_rew_mean          | 294          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2329         |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 56           |\n",
      "|    total_timesteps      | 10321920     |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026277797 |\n",
      "|    clip_fraction        | 0.0205       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.383       |\n",
      "|    explained_variance   | 0.995        |\n",
      "|    learning_rate        | 0.00127      |\n",
      "|    loss                 | 11.3         |\n",
      "|    n_updates            | 6266         |\n",
      "|    policy_gradient_loss | -0.00171     |\n",
      "|    value_loss           | 16.4         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 174          |\n",
      "|    ep_rew_mean          | 291          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2351         |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 69           |\n",
      "|    total_timesteps      | 10354688     |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033861178 |\n",
      "|    clip_fraction        | 0.0261       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.383       |\n",
      "|    explained_variance   | 0.984        |\n",
      "|    learning_rate        | 0.00127      |\n",
      "|    loss                 | 7.9          |\n",
      "|    n_updates            | 6289         |\n",
      "|    policy_gradient_loss | -0.00178     |\n",
      "|    value_loss           | 19.3         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 168          |\n",
      "|    ep_rew_mean          | 290          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2376         |\n",
      "|    iterations           | 6            |\n",
      "|    time_elapsed         | 82           |\n",
      "|    total_timesteps      | 10387456     |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0028675152 |\n",
      "|    clip_fraction        | 0.0279       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.388       |\n",
      "|    explained_variance   | 0.998        |\n",
      "|    learning_rate        | 0.00127      |\n",
      "|    loss                 | 1.39         |\n",
      "|    n_updates            | 6312         |\n",
      "|    policy_gradient_loss | -0.00164     |\n",
      "|    value_loss           | 2.38         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 170          |\n",
      "|    ep_rew_mean          | 293          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2355         |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 97           |\n",
      "|    total_timesteps      | 10420224     |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026611055 |\n",
      "|    clip_fraction        | 0.0217       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.388       |\n",
      "|    explained_variance   | 0.983        |\n",
      "|    learning_rate        | 0.00127      |\n",
      "|    loss                 | 29.2         |\n",
      "|    n_updates            | 6335         |\n",
      "|    policy_gradient_loss | -0.00144     |\n",
      "|    value_loss           | 48.8         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 170          |\n",
      "|    ep_rew_mean          | 288          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2238         |\n",
      "|    iterations           | 8            |\n",
      "|    time_elapsed         | 117          |\n",
      "|    total_timesteps      | 10452992     |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037549147 |\n",
      "|    clip_fraction        | 0.034        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.397       |\n",
      "|    explained_variance   | 0.997        |\n",
      "|    learning_rate        | 0.00127      |\n",
      "|    loss                 | 2.31         |\n",
      "|    n_updates            | 6358         |\n",
      "|    policy_gradient_loss | -0.00188     |\n",
      "|    value_loss           | 4.29         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 171          |\n",
      "|    ep_rew_mean          | 294          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2152         |\n",
      "|    iterations           | 9            |\n",
      "|    time_elapsed         | 137          |\n",
      "|    total_timesteps      | 10485760     |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041016024 |\n",
      "|    clip_fraction        | 0.0354       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.39        |\n",
      "|    explained_variance   | 0.984        |\n",
      "|    learning_rate        | 0.00127      |\n",
      "|    loss                 | 34.4         |\n",
      "|    n_updates            | 6381         |\n",
      "|    policy_gradient_loss | -0.00234     |\n",
      "|    value_loss           | 69.8         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 170         |\n",
      "|    ep_rew_mean          | 287         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 2058        |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 159         |\n",
      "|    total_timesteps      | 10518528    |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003249661 |\n",
      "|    clip_fraction        | 0.0299      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.379      |\n",
      "|    explained_variance   | 0.992       |\n",
      "|    learning_rate        | 0.00127     |\n",
      "|    loss                 | 32.2        |\n",
      "|    n_updates            | 6404        |\n",
      "|    policy_gradient_loss | -0.00166    |\n",
      "|    value_loss           | 37.6        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 171          |\n",
      "|    ep_rew_mean          | 293          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2005         |\n",
      "|    iterations           | 11           |\n",
      "|    time_elapsed         | 179          |\n",
      "|    total_timesteps      | 10551296     |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032936677 |\n",
      "|    clip_fraction        | 0.0324       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.39        |\n",
      "|    explained_variance   | 0.994        |\n",
      "|    learning_rate        | 0.00127      |\n",
      "|    loss                 | 22.2         |\n",
      "|    n_updates            | 6427         |\n",
      "|    policy_gradient_loss | -0.00147     |\n",
      "|    value_loss           | 25.9         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 166          |\n",
      "|    ep_rew_mean          | 290          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1971         |\n",
      "|    iterations           | 12           |\n",
      "|    time_elapsed         | 199          |\n",
      "|    total_timesteps      | 10584064     |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0039755455 |\n",
      "|    clip_fraction        | 0.0385       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.397       |\n",
      "|    explained_variance   | 0.995        |\n",
      "|    learning_rate        | 0.00127      |\n",
      "|    loss                 | 3.09         |\n",
      "|    n_updates            | 6450         |\n",
      "|    policy_gradient_loss | -0.00262     |\n",
      "|    value_loss           | 6.18         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 171         |\n",
      "|    ep_rew_mean          | 292         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1946        |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 218         |\n",
      "|    total_timesteps      | 10616832    |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003942062 |\n",
      "|    clip_fraction        | 0.0402      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.405      |\n",
      "|    explained_variance   | 0.999       |\n",
      "|    learning_rate        | 0.00127     |\n",
      "|    loss                 | 1.11        |\n",
      "|    n_updates            | 6473        |\n",
      "|    policy_gradient_loss | -0.00231    |\n",
      "|    value_loss           | 1.81        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 169          |\n",
      "|    ep_rew_mean          | 293          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1945         |\n",
      "|    iterations           | 14           |\n",
      "|    time_elapsed         | 235          |\n",
      "|    total_timesteps      | 10649600     |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041942485 |\n",
      "|    clip_fraction        | 0.0448       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.395       |\n",
      "|    explained_variance   | 1            |\n",
      "|    learning_rate        | 0.00127      |\n",
      "|    loss                 | 1.13         |\n",
      "|    n_updates            | 6496         |\n",
      "|    policy_gradient_loss | -0.00329     |\n",
      "|    value_loss           | 1.28         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 170          |\n",
      "|    ep_rew_mean          | 294          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1952         |\n",
      "|    iterations           | 15           |\n",
      "|    time_elapsed         | 251          |\n",
      "|    total_timesteps      | 10682368     |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037198237 |\n",
      "|    clip_fraction        | 0.0366       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.385       |\n",
      "|    explained_variance   | 1            |\n",
      "|    learning_rate        | 0.00127      |\n",
      "|    loss                 | 1.06         |\n",
      "|    n_updates            | 6519         |\n",
      "|    policy_gradient_loss | -0.00369     |\n",
      "|    value_loss           | 1.32         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 166          |\n",
      "|    ep_rew_mean          | 282          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1918         |\n",
      "|    iterations           | 16           |\n",
      "|    time_elapsed         | 273          |\n",
      "|    total_timesteps      | 10715136     |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038209856 |\n",
      "|    clip_fraction        | 0.0402       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.403       |\n",
      "|    explained_variance   | 1            |\n",
      "|    learning_rate        | 0.00127      |\n",
      "|    loss                 | 1.13         |\n",
      "|    n_updates            | 6542         |\n",
      "|    policy_gradient_loss | -0.00316     |\n",
      "|    value_loss           | 1.51         |\n",
      "------------------------------------------\n",
      "Restarting PPO with new hyperparameters: {'ent_coef': 0.003113711996695648, 'vf_coef': 0.8825715557391307, 'n_epochs': 23, 'threshold': 161, 'modified_length': True, 'modified_reward': True, 'lr': 0.0012749372513232453}\n",
      "Logging to ./final_tensorboard/PPO_0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/home/ruben/miniconda3/envs/rapids-23.10/lib/python3.10/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/home/ruben/miniconda3/envs/rapids-23.10/lib/python3.10/multiprocessing/spawn.py\", line 126, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "AttributeError: Can't get attribute 'evaluate' on <module '__main__' (built-in)>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 167      |\n",
      "|    ep_rew_mean     | 289      |\n",
      "| time/              |          |\n",
      "|    fps             | 3033     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 10       |\n",
      "|    total_timesteps | 10747904 |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 167         |\n",
      "|    ep_rew_mean          | 277         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 2736        |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 23          |\n",
      "|    total_timesteps      | 10780672    |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003178773 |\n",
      "|    clip_fraction        | 0.0284      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.382      |\n",
      "|    explained_variance   | 0.982       |\n",
      "|    learning_rate        | 0.00127     |\n",
      "|    loss                 | 45.9        |\n",
      "|    n_updates            | 6588        |\n",
      "|    policy_gradient_loss | -0.00159    |\n",
      "|    value_loss           | 83.7        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 166         |\n",
      "|    ep_rew_mean          | 282         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 2556        |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 38          |\n",
      "|    total_timesteps      | 10813440    |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002808611 |\n",
      "|    clip_fraction        | 0.0221      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.392      |\n",
      "|    explained_variance   | 0.967       |\n",
      "|    learning_rate        | 0.00127     |\n",
      "|    loss                 | 94.9        |\n",
      "|    n_updates            | 6611        |\n",
      "|    policy_gradient_loss | -0.00184    |\n",
      "|    value_loss           | 135         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 170         |\n",
      "|    ep_rew_mean          | 289         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 2402        |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 54          |\n",
      "|    total_timesteps      | 10846208    |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.002553419 |\n",
      "|    clip_fraction        | 0.0225      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.386      |\n",
      "|    explained_variance   | 0.978       |\n",
      "|    learning_rate        | 0.00127     |\n",
      "|    loss                 | 61.2        |\n",
      "|    n_updates            | 6634        |\n",
      "|    policy_gradient_loss | -0.00218    |\n",
      "|    value_loss           | 95.6        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 167          |\n",
      "|    ep_rew_mean          | 289          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2310         |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 70           |\n",
      "|    total_timesteps      | 10878976     |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036873166 |\n",
      "|    clip_fraction        | 0.0255       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.393       |\n",
      "|    explained_variance   | 0.985        |\n",
      "|    learning_rate        | 0.00127      |\n",
      "|    loss                 | 46.4         |\n",
      "|    n_updates            | 6657         |\n",
      "|    policy_gradient_loss | -0.00147     |\n",
      "|    value_loss           | 46.5         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 170          |\n",
      "|    ep_rew_mean          | 289          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2290         |\n",
      "|    iterations           | 6            |\n",
      "|    time_elapsed         | 85           |\n",
      "|    total_timesteps      | 10911744     |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0044365567 |\n",
      "|    clip_fraction        | 0.0399       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.39        |\n",
      "|    explained_variance   | 0.998        |\n",
      "|    learning_rate        | 0.00127      |\n",
      "|    loss                 | 2.38         |\n",
      "|    n_updates            | 6680         |\n",
      "|    policy_gradient_loss | -0.00256     |\n",
      "|    value_loss           | 4.06         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 168          |\n",
      "|    ep_rew_mean          | 291          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2303         |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 99           |\n",
      "|    total_timesteps      | 10944512     |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0048794514 |\n",
      "|    clip_fraction        | 0.0442       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.401       |\n",
      "|    explained_variance   | 0.994        |\n",
      "|    learning_rate        | 0.00127      |\n",
      "|    loss                 | 8.57         |\n",
      "|    n_updates            | 6703         |\n",
      "|    policy_gradient_loss | -0.00287     |\n",
      "|    value_loss           | 16.1         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 168          |\n",
      "|    ep_rew_mean          | 288          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2220         |\n",
      "|    iterations           | 8            |\n",
      "|    time_elapsed         | 118          |\n",
      "|    total_timesteps      | 10977280     |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055892854 |\n",
      "|    clip_fraction        | 0.0449       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.398       |\n",
      "|    explained_variance   | 0.992        |\n",
      "|    learning_rate        | 0.00127      |\n",
      "|    loss                 | 50.1         |\n",
      "|    n_updates            | 6726         |\n",
      "|    policy_gradient_loss | -0.00182     |\n",
      "|    value_loss           | 39.2         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 171          |\n",
      "|    ep_rew_mean          | 286          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2202         |\n",
      "|    iterations           | 9            |\n",
      "|    time_elapsed         | 133          |\n",
      "|    total_timesteps      | 11010048     |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0038063722 |\n",
      "|    clip_fraction        | 0.0383       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.386       |\n",
      "|    explained_variance   | 0.994        |\n",
      "|    learning_rate        | 0.00127      |\n",
      "|    loss                 | 28           |\n",
      "|    n_updates            | 6749         |\n",
      "|    policy_gradient_loss | -0.00217     |\n",
      "|    value_loss           | 32.5         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 175          |\n",
      "|    ep_rew_mean          | 294          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2188         |\n",
      "|    iterations           | 10           |\n",
      "|    time_elapsed         | 149          |\n",
      "|    total_timesteps      | 11042816     |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035284613 |\n",
      "|    clip_fraction        | 0.0264       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.402       |\n",
      "|    explained_variance   | 0.973        |\n",
      "|    learning_rate        | 0.00127      |\n",
      "|    loss                 | 34.1         |\n",
      "|    n_updates            | 6772         |\n",
      "|    policy_gradient_loss | -0.00191     |\n",
      "|    value_loss           | 67           |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 166          |\n",
      "|    ep_rew_mean          | 289          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2159         |\n",
      "|    iterations           | 11           |\n",
      "|    time_elapsed         | 166          |\n",
      "|    total_timesteps      | 11075584     |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036213566 |\n",
      "|    clip_fraction        | 0.0391       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.38        |\n",
      "|    explained_variance   | 0.994        |\n",
      "|    learning_rate        | 0.00127      |\n",
      "|    loss                 | 4.8          |\n",
      "|    n_updates            | 6795         |\n",
      "|    policy_gradient_loss | -0.00272     |\n",
      "|    value_loss           | 9.94         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 172          |\n",
      "|    ep_rew_mean          | 284          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2157         |\n",
      "|    iterations           | 12           |\n",
      "|    time_elapsed         | 182          |\n",
      "|    total_timesteps      | 11108352     |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042223535 |\n",
      "|    clip_fraction        | 0.0367       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.379       |\n",
      "|    explained_variance   | 0.994        |\n",
      "|    learning_rate        | 0.00127      |\n",
      "|    loss                 | 28.5         |\n",
      "|    n_updates            | 6818         |\n",
      "|    policy_gradient_loss | -0.0017      |\n",
      "|    value_loss           | 27.1         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 168         |\n",
      "|    ep_rew_mean          | 289         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 2150        |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 198         |\n",
      "|    total_timesteps      | 11141120    |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003868456 |\n",
      "|    clip_fraction        | 0.0336      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.381      |\n",
      "|    explained_variance   | 0.99        |\n",
      "|    learning_rate        | 0.00127     |\n",
      "|    loss                 | 55          |\n",
      "|    n_updates            | 6841        |\n",
      "|    policy_gradient_loss | -0.00185    |\n",
      "|    value_loss           | 42          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 167         |\n",
      "|    ep_rew_mean          | 291         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 2151        |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 213         |\n",
      "|    total_timesteps      | 11173888    |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003062284 |\n",
      "|    clip_fraction        | 0.0329      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.381      |\n",
      "|    explained_variance   | 0.993       |\n",
      "|    learning_rate        | 0.00127     |\n",
      "|    loss                 | 12.7        |\n",
      "|    n_updates            | 6864        |\n",
      "|    policy_gradient_loss | -0.00155    |\n",
      "|    value_loss           | 24.2        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 170         |\n",
      "|    ep_rew_mean          | 289         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 2158        |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 227         |\n",
      "|    total_timesteps      | 11206656    |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004617512 |\n",
      "|    clip_fraction        | 0.0393      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.393      |\n",
      "|    explained_variance   | 0.992       |\n",
      "|    learning_rate        | 0.00127     |\n",
      "|    loss                 | 29.7        |\n",
      "|    n_updates            | 6887        |\n",
      "|    policy_gradient_loss | -0.00178    |\n",
      "|    value_loss           | 32.1        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 167          |\n",
      "|    ep_rew_mean          | 287          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2163         |\n",
      "|    iterations           | 16           |\n",
      "|    time_elapsed         | 242          |\n",
      "|    total_timesteps      | 11239424     |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031816901 |\n",
      "|    clip_fraction        | 0.0258       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.386       |\n",
      "|    explained_variance   | 0.975        |\n",
      "|    learning_rate        | 0.00127      |\n",
      "|    loss                 | 53.1         |\n",
      "|    n_updates            | 6910         |\n",
      "|    policy_gradient_loss | -0.00213     |\n",
      "|    value_loss           | 89           |\n",
      "------------------------------------------\n",
      "Restarting PPO with new hyperparameters: {'ent_coef': 0.003113711996695648, 'vf_coef': 0.8825715557391307, 'n_epochs': 23, 'threshold': 161, 'modified_length': True, 'modified_reward': True, 'lr': 0.0012749372513232453}\n",
      "Logging to ./final_tensorboard/PPO_0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/home/ruben/miniconda3/envs/rapids-23.10/lib/python3.10/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/home/ruben/miniconda3/envs/rapids-23.10/lib/python3.10/multiprocessing/spawn.py\", line 126, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "AttributeError: Can't get attribute 'evaluate' on <module '__main__' (built-in)>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 169      |\n",
      "|    ep_rew_mean     | 291      |\n",
      "| time/              |          |\n",
      "|    fps             | 2864     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 11       |\n",
      "|    total_timesteps | 11272192 |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 169         |\n",
      "|    ep_rew_mean          | 285         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 2592        |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 25          |\n",
      "|    total_timesteps      | 11304960    |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003693235 |\n",
      "|    clip_fraction        | 0.0332      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.413      |\n",
      "|    explained_variance   | 0.991       |\n",
      "|    learning_rate        | 0.00127     |\n",
      "|    loss                 | 7.17        |\n",
      "|    n_updates            | 6956        |\n",
      "|    policy_gradient_loss | -0.00152    |\n",
      "|    value_loss           | 12.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 172         |\n",
      "|    ep_rew_mean          | 293         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 2449        |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 40          |\n",
      "|    total_timesteps      | 11337728    |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004234641 |\n",
      "|    clip_fraction        | 0.0412      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.419      |\n",
      "|    explained_variance   | 0.994       |\n",
      "|    learning_rate        | 0.00127     |\n",
      "|    loss                 | 22.4        |\n",
      "|    n_updates            | 6979        |\n",
      "|    policy_gradient_loss | -0.00183    |\n",
      "|    value_loss           | 29.8        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 169         |\n",
      "|    ep_rew_mean          | 285         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 2333        |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 56          |\n",
      "|    total_timesteps      | 11370496    |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004411106 |\n",
      "|    clip_fraction        | 0.0346      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.407      |\n",
      "|    explained_variance   | 0.98        |\n",
      "|    learning_rate        | 0.00127     |\n",
      "|    loss                 | 52.6        |\n",
      "|    n_updates            | 7002        |\n",
      "|    policy_gradient_loss | -0.00289    |\n",
      "|    value_loss           | 75.1        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 171          |\n",
      "|    ep_rew_mean          | 291          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2259         |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 72           |\n",
      "|    total_timesteps      | 11403264     |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026709565 |\n",
      "|    clip_fraction        | 0.024        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.404       |\n",
      "|    explained_variance   | 0.974        |\n",
      "|    learning_rate        | 0.00127      |\n",
      "|    loss                 | 72.4         |\n",
      "|    n_updates            | 7025         |\n",
      "|    policy_gradient_loss | -0.00162     |\n",
      "|    value_loss           | 98.5         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 172          |\n",
      "|    ep_rew_mean          | 289          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2168         |\n",
      "|    iterations           | 6            |\n",
      "|    time_elapsed         | 90           |\n",
      "|    total_timesteps      | 11436032     |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0040849643 |\n",
      "|    clip_fraction        | 0.0336       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.403       |\n",
      "|    explained_variance   | 0.985        |\n",
      "|    learning_rate        | 0.00127      |\n",
      "|    loss                 | 22.3         |\n",
      "|    n_updates            | 7048         |\n",
      "|    policy_gradient_loss | -0.00202     |\n",
      "|    value_loss           | 35.9         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 172          |\n",
      "|    ep_rew_mean          | 290          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2191         |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 104          |\n",
      "|    total_timesteps      | 11468800     |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0065608863 |\n",
      "|    clip_fraction        | 0.0453       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.416       |\n",
      "|    explained_variance   | 0.982        |\n",
      "|    learning_rate        | 0.00127      |\n",
      "|    loss                 | 46.9         |\n",
      "|    n_updates            | 7071         |\n",
      "|    policy_gradient_loss | -0.00303     |\n",
      "|    value_loss           | 65.5         |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 173        |\n",
      "|    ep_rew_mean          | 292        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 2229       |\n",
      "|    iterations           | 8          |\n",
      "|    time_elapsed         | 117        |\n",
      "|    total_timesteps      | 11501568   |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00382334 |\n",
      "|    clip_fraction        | 0.0345     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.415     |\n",
      "|    explained_variance   | 0.977      |\n",
      "|    learning_rate        | 0.00127    |\n",
      "|    loss                 | 90.4       |\n",
      "|    n_updates            | 7094       |\n",
      "|    policy_gradient_loss | -0.002     |\n",
      "|    value_loss           | 89.9       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 168         |\n",
      "|    ep_rew_mean          | 288         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 2263        |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 130         |\n",
      "|    total_timesteps      | 11534336    |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003924586 |\n",
      "|    clip_fraction        | 0.0379      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.409      |\n",
      "|    explained_variance   | 0.987       |\n",
      "|    learning_rate        | 0.00127     |\n",
      "|    loss                 | 44.5        |\n",
      "|    n_updates            | 7117        |\n",
      "|    policy_gradient_loss | -0.00333    |\n",
      "|    value_loss           | 55.9        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 170          |\n",
      "|    ep_rew_mean          | 293          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2293         |\n",
      "|    iterations           | 10           |\n",
      "|    time_elapsed         | 142          |\n",
      "|    total_timesteps      | 11567104     |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0032894006 |\n",
      "|    clip_fraction        | 0.0355       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.412       |\n",
      "|    explained_variance   | 0.974        |\n",
      "|    learning_rate        | 0.00127      |\n",
      "|    loss                 | 72.4         |\n",
      "|    n_updates            | 7140         |\n",
      "|    policy_gradient_loss | -0.00252     |\n",
      "|    value_loss           | 98.6         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 169         |\n",
      "|    ep_rew_mean          | 292         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 2306        |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 156         |\n",
      "|    total_timesteps      | 11599872    |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007900494 |\n",
      "|    clip_fraction        | 0.0348      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.416      |\n",
      "|    explained_variance   | 0.994       |\n",
      "|    learning_rate        | 0.00127     |\n",
      "|    loss                 | 4.51        |\n",
      "|    n_updates            | 7163        |\n",
      "|    policy_gradient_loss | -0.00117    |\n",
      "|    value_loss           | 8.38        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 175          |\n",
      "|    ep_rew_mean          | 293          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2284         |\n",
      "|    iterations           | 12           |\n",
      "|    time_elapsed         | 172          |\n",
      "|    total_timesteps      | 11632640     |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0065385336 |\n",
      "|    clip_fraction        | 0.0608       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.434       |\n",
      "|    explained_variance   | 0.999        |\n",
      "|    learning_rate        | 0.00127      |\n",
      "|    loss                 | 1.22         |\n",
      "|    n_updates            | 7186         |\n",
      "|    policy_gradient_loss | -0.00393     |\n",
      "|    value_loss           | 1.69         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 171          |\n",
      "|    ep_rew_mean          | 292          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2296         |\n",
      "|    iterations           | 13           |\n",
      "|    time_elapsed         | 185          |\n",
      "|    total_timesteps      | 11665408     |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0076788487 |\n",
      "|    clip_fraction        | 0.0468       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.448       |\n",
      "|    explained_variance   | 0.995        |\n",
      "|    learning_rate        | 0.00127      |\n",
      "|    loss                 | 11.6         |\n",
      "|    n_updates            | 7209         |\n",
      "|    policy_gradient_loss | -0.00225     |\n",
      "|    value_loss           | 21.7         |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 168        |\n",
      "|    ep_rew_mean          | 289        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 2317       |\n",
      "|    iterations           | 14         |\n",
      "|    time_elapsed         | 197        |\n",
      "|    total_timesteps      | 11698176   |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00376182 |\n",
      "|    clip_fraction        | 0.0303     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.436     |\n",
      "|    explained_variance   | 0.961      |\n",
      "|    learning_rate        | 0.00127    |\n",
      "|    loss                 | 137        |\n",
      "|    n_updates            | 7232       |\n",
      "|    policy_gradient_loss | -0.00214   |\n",
      "|    value_loss           | 211        |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 169          |\n",
      "|    ep_rew_mean          | 293          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2326         |\n",
      "|    iterations           | 15           |\n",
      "|    time_elapsed         | 211          |\n",
      "|    total_timesteps      | 11730944     |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027571344 |\n",
      "|    clip_fraction        | 0.0248       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.429       |\n",
      "|    explained_variance   | 0.987        |\n",
      "|    learning_rate        | 0.00127      |\n",
      "|    loss                 | 55.8         |\n",
      "|    n_updates            | 7255         |\n",
      "|    policy_gradient_loss | -0.00146     |\n",
      "|    value_loss           | 61.6         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 166          |\n",
      "|    ep_rew_mean          | 287          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2308         |\n",
      "|    iterations           | 16           |\n",
      "|    time_elapsed         | 227          |\n",
      "|    total_timesteps      | 11763712     |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0045628636 |\n",
      "|    clip_fraction        | 0.0335       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.416       |\n",
      "|    explained_variance   | 0.994        |\n",
      "|    learning_rate        | 0.00127      |\n",
      "|    loss                 | 12           |\n",
      "|    n_updates            | 7278         |\n",
      "|    policy_gradient_loss | -0.00184     |\n",
      "|    value_loss           | 16.5         |\n",
      "------------------------------------------\n",
      "Restarting PPO with new hyperparameters: {'ent_coef': 0.003113711996695648, 'vf_coef': 0.8825715557391307, 'n_epochs': 23, 'threshold': 161, 'modified_length': True, 'modified_reward': True, 'lr': 0.0012749372513232453}\n",
      "Logging to ./final_tensorboard/PPO_0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/home/ruben/miniconda3/envs/rapids-23.10/lib/python3.10/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/home/ruben/miniconda3/envs/rapids-23.10/lib/python3.10/multiprocessing/spawn.py\", line 126, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "AttributeError: Can't get attribute 'evaluate' on <module '__main__' (built-in)>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 167      |\n",
      "|    ep_rew_mean     | 289      |\n",
      "| time/              |          |\n",
      "|    fps             | 2840     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 11       |\n",
      "|    total_timesteps | 11796480 |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 169          |\n",
      "|    ep_rew_mean          | 285          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2738         |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 23           |\n",
      "|    total_timesteps      | 11829248     |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034322706 |\n",
      "|    clip_fraction        | 0.0288       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.428       |\n",
      "|    explained_variance   | 0.983        |\n",
      "|    learning_rate        | 0.00127      |\n",
      "|    loss                 | 63.9         |\n",
      "|    n_updates            | 7323         |\n",
      "|    policy_gradient_loss | -0.00157     |\n",
      "|    value_loss           | 69.4         |\n",
      "------------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 171        |\n",
      "|    ep_rew_mean          | 293        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 2700       |\n",
      "|    iterations           | 3          |\n",
      "|    time_elapsed         | 36         |\n",
      "|    total_timesteps      | 11862016   |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00345578 |\n",
      "|    clip_fraction        | 0.0341     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.43      |\n",
      "|    explained_variance   | 0.981      |\n",
      "|    learning_rate        | 0.00127    |\n",
      "|    loss                 | 58         |\n",
      "|    n_updates            | 7345       |\n",
      "|    policy_gradient_loss | -0.00207   |\n",
      "|    value_loss           | 71.7       |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 170          |\n",
      "|    ep_rew_mean          | 292          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2666         |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 49           |\n",
      "|    total_timesteps      | 11894784     |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053698127 |\n",
      "|    clip_fraction        | 0.0604       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.435       |\n",
      "|    explained_variance   | 0.997        |\n",
      "|    learning_rate        | 0.00127      |\n",
      "|    loss                 | 2.53         |\n",
      "|    n_updates            | 7367         |\n",
      "|    policy_gradient_loss | -0.00242     |\n",
      "|    value_loss           | 4.31         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 170          |\n",
      "|    ep_rew_mean          | 287          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2568         |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 63           |\n",
      "|    total_timesteps      | 11927552     |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050397525 |\n",
      "|    clip_fraction        | 0.0454       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.432       |\n",
      "|    explained_variance   | 0.993        |\n",
      "|    learning_rate        | 0.00127      |\n",
      "|    loss                 | 22.9         |\n",
      "|    n_updates            | 7389         |\n",
      "|    policy_gradient_loss | -0.00213     |\n",
      "|    value_loss           | 28.4         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 169          |\n",
      "|    ep_rew_mean          | 289          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2468         |\n",
      "|    iterations           | 6            |\n",
      "|    time_elapsed         | 79           |\n",
      "|    total_timesteps      | 11960320     |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034949225 |\n",
      "|    clip_fraction        | 0.031        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.425       |\n",
      "|    explained_variance   | 0.976        |\n",
      "|    learning_rate        | 0.00127      |\n",
      "|    loss                 | 93.6         |\n",
      "|    n_updates            | 7411         |\n",
      "|    policy_gradient_loss | -0.00215     |\n",
      "|    value_loss           | 114          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 172         |\n",
      "|    ep_rew_mean          | 289         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 2385        |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 96          |\n",
      "|    total_timesteps      | 11993088    |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004592374 |\n",
      "|    clip_fraction        | 0.0467      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.415      |\n",
      "|    explained_variance   | 0.998       |\n",
      "|    learning_rate        | 0.00127     |\n",
      "|    loss                 | 2.62        |\n",
      "|    n_updates            | 7433        |\n",
      "|    policy_gradient_loss | -0.0029     |\n",
      "|    value_loss           | 3.98        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 166          |\n",
      "|    ep_rew_mean          | 292          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2341         |\n",
      "|    iterations           | 8            |\n",
      "|    time_elapsed         | 111          |\n",
      "|    total_timesteps      | 12025856     |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033750734 |\n",
      "|    clip_fraction        | 0.0323       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.42        |\n",
      "|    explained_variance   | 0.988        |\n",
      "|    learning_rate        | 0.00127      |\n",
      "|    loss                 | 19.4         |\n",
      "|    n_updates            | 7455         |\n",
      "|    policy_gradient_loss | -0.00172     |\n",
      "|    value_loss           | 37.5         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 171          |\n",
      "|    ep_rew_mean          | 289          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2337         |\n",
      "|    iterations           | 9            |\n",
      "|    time_elapsed         | 126          |\n",
      "|    total_timesteps      | 12058624     |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0034534456 |\n",
      "|    clip_fraction        | 0.0309       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.421       |\n",
      "|    explained_variance   | 0.992        |\n",
      "|    learning_rate        | 0.00127      |\n",
      "|    loss                 | 5.18         |\n",
      "|    n_updates            | 7477         |\n",
      "|    policy_gradient_loss | -0.00261     |\n",
      "|    value_loss           | 15.1         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 170          |\n",
      "|    ep_rew_mean          | 290          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2223         |\n",
      "|    iterations           | 10           |\n",
      "|    time_elapsed         | 147          |\n",
      "|    total_timesteps      | 12091392     |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0040072873 |\n",
      "|    clip_fraction        | 0.0333       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.419       |\n",
      "|    explained_variance   | 0.985        |\n",
      "|    learning_rate        | 0.00127      |\n",
      "|    loss                 | 23.8         |\n",
      "|    n_updates            | 7499         |\n",
      "|    policy_gradient_loss | -0.00128     |\n",
      "|    value_loss           | 40.4         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 171          |\n",
      "|    ep_rew_mean          | 291          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2187         |\n",
      "|    iterations           | 11           |\n",
      "|    time_elapsed         | 164          |\n",
      "|    total_timesteps      | 12124160     |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0054487446 |\n",
      "|    clip_fraction        | 0.0403       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.412       |\n",
      "|    explained_variance   | 0.999        |\n",
      "|    learning_rate        | 0.00127      |\n",
      "|    loss                 | 1.69         |\n",
      "|    n_updates            | 7521         |\n",
      "|    policy_gradient_loss | -0.00255     |\n",
      "|    value_loss           | 2.52         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 170          |\n",
      "|    ep_rew_mean          | 291          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2175         |\n",
      "|    iterations           | 12           |\n",
      "|    time_elapsed         | 180          |\n",
      "|    total_timesteps      | 12156928     |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0037865192 |\n",
      "|    clip_fraction        | 0.0318       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.428       |\n",
      "|    explained_variance   | 0.994        |\n",
      "|    learning_rate        | 0.00127      |\n",
      "|    loss                 | 20.7         |\n",
      "|    n_updates            | 7543         |\n",
      "|    policy_gradient_loss | -0.00127     |\n",
      "|    value_loss           | 25.5         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 172         |\n",
      "|    ep_rew_mean          | 290         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 2142        |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 198         |\n",
      "|    total_timesteps      | 12189696    |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005078363 |\n",
      "|    clip_fraction        | 0.0487      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.422      |\n",
      "|    explained_variance   | 0.999       |\n",
      "|    learning_rate        | 0.00127     |\n",
      "|    loss                 | 1.46        |\n",
      "|    n_updates            | 7565        |\n",
      "|    policy_gradient_loss | -0.00334    |\n",
      "|    value_loss           | 1.94        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 168         |\n",
      "|    ep_rew_mean          | 292         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 2108        |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 217         |\n",
      "|    total_timesteps      | 12222464    |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004743481 |\n",
      "|    clip_fraction        | 0.0387      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.439      |\n",
      "|    explained_variance   | 0.992       |\n",
      "|    learning_rate        | 0.00127     |\n",
      "|    loss                 | 45.2        |\n",
      "|    n_updates            | 7587        |\n",
      "|    policy_gradient_loss | -0.00178    |\n",
      "|    value_loss           | 42.4        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Actual training\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining with PPO...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mtrain_with_ppo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20_000_000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 159\u001b[0m, in \u001b[0;36mtrain_with_ppo\u001b[0;34m(total_timesteps)\u001b[0m\n\u001b[1;32m    148\u001b[0m         model \u001b[38;5;241m=\u001b[39m PPO\u001b[38;5;241m.\u001b[39mload(\n\u001b[1;32m    149\u001b[0m             MODEL_PATH,\n\u001b[1;32m    150\u001b[0m             env\u001b[38;5;241m=\u001b[39menv,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    153\u001b[0m             policy_kwargs\u001b[38;5;241m=\u001b[39mpolicy_kwargs\n\u001b[1;32m    154\u001b[0m         )\n\u001b[1;32m    156\u001b[0m \u001b[38;5;66;03m# -----------------------------------------------\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;66;03m# Train the model for the current step interval\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;66;03m# -----------------------------------------------\u001b[39;00m\n\u001b[0;32m--> 159\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m current_timesteps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m train_step  \u001b[38;5;66;03m# Update the total timesteps count\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;66;03m# -----------------------------------------------\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;66;03m# Save the model after training iteration\u001b[39;00m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;66;03m# -----------------------------------------------\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/rapids-23.10/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py:311\u001b[0m, in \u001b[0;36mPPO.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[1;32m    304\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    309\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    310\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfPPO:\n\u001b[0;32m--> 311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/rapids-23.10/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py:323\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[0;32m--> 323\u001b[0m     continue_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_rollout_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    325\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m continue_training:\n\u001b[1;32m    326\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/rapids-23.10/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py:218\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.collect_rollouts\u001b[0;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    214\u001b[0m         \u001b[38;5;66;03m# Otherwise, clip the actions to avoid out of bound error\u001b[39;00m\n\u001b[1;32m    215\u001b[0m         \u001b[38;5;66;03m# as we are sampling from an unbounded Gaussian distribution\u001b[39;00m\n\u001b[1;32m    216\u001b[0m         clipped_actions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mclip(actions, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mlow, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mhigh)\n\u001b[0;32m--> 218\u001b[0m new_obs, rewards, dones, infos \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclipped_actions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mnum_envs\n\u001b[1;32m    222\u001b[0m \u001b[38;5;66;03m# Give access to local variables\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/rapids-23.10/lib/python3.10/site-packages/stable_baselines3/common/vec_env/base_vec_env.py:207\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;124;03mStep the environments with the given action\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \n\u001b[1;32m    203\u001b[0m \u001b[38;5;124;03m:param actions: the action\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;124;03m:return: observation, reward, done, information\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_async(actions)\n\u001b[0;32m--> 207\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/rapids-23.10/lib/python3.10/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:59\u001b[0m, in \u001b[0;36mDummyVecEnv.step_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m VecEnvStepReturn:\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;66;03m# Avoid circular imports\u001b[39;00m\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m env_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_envs):\n\u001b[0;32m---> 59\u001b[0m         obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_rews[env_idx], terminated, truncated, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_infos[env_idx] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menvs\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[assignment]\u001b[39;49;00m\n\u001b[1;32m     60\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactions\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m         \u001b[38;5;66;03m# convert to SB3 VecEnv api\u001b[39;00m\n\u001b[1;32m     63\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_dones[env_idx] \u001b[38;5;241m=\u001b[39m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n",
      "File \u001b[0;32m~/miniconda3/envs/rapids-23.10/lib/python3.10/site-packages/stable_baselines3/common/monitor.py:94\u001b[0m, in \u001b[0;36mMonitor.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneeds_reset:\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTried to step environment that needs reset\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 94\u001b[0m observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrewards\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mfloat\u001b[39m(reward))\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated:\n",
      "File \u001b[0;32m~/miniconda3/envs/rapids-23.10/lib/python3.10/site-packages/gymnasium/wrappers/common.py:125\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: ActType\n\u001b[1;32m    114\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[ObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[1;32m    115\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    123\u001b[0m \n\u001b[1;32m    124\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 125\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[0;32m~/miniconda3/envs/rapids-23.10/lib/python3.10/site-packages/gymnasium/wrappers/common.py:393\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 393\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/rapids-23.10/lib/python3.10/site-packages/gymnasium/core.py:322\u001b[0m, in \u001b[0;36mWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: WrapperActType\n\u001b[1;32m    320\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[1;32m    321\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Uses the :meth:`step` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 322\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/rapids-23.10/lib/python3.10/site-packages/gymnasium/wrappers/common.py:285\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/rapids-23.10/lib/python3.10/site-packages/gymnasium/envs/box2d/lunar_lander.py:621\u001b[0m, in \u001b[0;36mLunarLander.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    607\u001b[0m         p\u001b[38;5;241m.\u001b[39mApplyLinearImpulse(\n\u001b[1;32m    608\u001b[0m             (\n\u001b[1;32m    609\u001b[0m                 ox \u001b[38;5;241m*\u001b[39m SIDE_ENGINE_POWER \u001b[38;5;241m*\u001b[39m s_power,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    613\u001b[0m             \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    614\u001b[0m         )\n\u001b[1;32m    615\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlander\u001b[38;5;241m.\u001b[39mApplyLinearImpulse(\n\u001b[1;32m    616\u001b[0m         (\u001b[38;5;241m-\u001b[39mox \u001b[38;5;241m*\u001b[39m SIDE_ENGINE_POWER \u001b[38;5;241m*\u001b[39m s_power, \u001b[38;5;241m-\u001b[39moy \u001b[38;5;241m*\u001b[39m SIDE_ENGINE_POWER \u001b[38;5;241m*\u001b[39m s_power),\n\u001b[1;32m    617\u001b[0m         impulse_pos,\n\u001b[1;32m    618\u001b[0m         \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    619\u001b[0m     )\n\u001b[0;32m--> 621\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mworld\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mStep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mFPS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    623\u001b[0m pos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlander\u001b[38;5;241m.\u001b[39mposition\n\u001b[1;32m    624\u001b[0m vel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlander\u001b[38;5;241m.\u001b[39mlinearVelocity\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Actual training\n",
    "print(\"Training with PPO...\")\n",
    "train_with_ppo(total_timesteps=500_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If you just want the evaluation visualization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Episode 1 reward: 307.2900, Steps= 177\n",
      "Episode 2 reward: 309.7749, Steps= 182\n",
      "Episode 3 reward: 264.4052, Steps= 140\n"
     ]
    }
   ],
   "source": [
    "evaluate(MODEL_PATH, 5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids-23.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
