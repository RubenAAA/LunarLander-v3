{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üîá Suppressing non-critical warnings for a cleaner output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "# Ignore specific warnings about PPO running on GPU (it's CPU-optimized)\n",
    "warnings.filterwarnings(\"ignore\", message=\"You are trying to run PPO on the GPU, but it is primarily intended to run on the CPU\")\n",
    "\n",
    "# Disable TensorFlow TensorRT GPU fallback (not needed for this implementation)\n",
    "os.environ[\"TF_TRT_ALLOW_GPU_FALLBACK\"] = \"0\"\n",
    "\n",
    "# Suppress TensorFlow debug logs\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "logging.getLogger('tensorflow').setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìå Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Standard libraries \n",
    "import time\n",
    "import json\n",
    "import random\n",
    "import multiprocessing\n",
    "\n",
    "## Numerical & deep learning libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "\n",
    "## Reinforcement Learning & Gym\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.callbacks import BaseCallback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üé≤ Setting Random Seed for Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1337  # Fixed seed value for consistent results\n",
    "\n",
    "# Set seed for Python's built-in random module\n",
    "random.seed(SEED)\n",
    "\n",
    "# Set seed for NumPy (affects all NumPy-based random operations)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Set seed for PyTorch (affects all tensor operations using randomization)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# Set seed for CUDA operations (ensures deterministic behavior on GPUs)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# Ensure deterministic computations in cuDNN (may affect performance slightly)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ö° Optimizing PyTorch Performance on CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable cuDNN auto-tuner to find the best algorithms for the current hardware\n",
    "torch.backends.cudnn.benchmark = True  \n",
    "\n",
    "# Ensure cuDNN is enabled (improves performance for convolutional operations)\n",
    "torch.backends.cudnn.enabled = True  \n",
    "\n",
    "# Set higher precision for float32 matrix multiplications (boosts performance)\n",
    "torch.set_float32_matmul_precision(\"high\")  \n",
    "\n",
    "# Free up unused GPU memory to avoid fragmentation issues\n",
    "torch.cuda.empty_cache()  \n",
    "\n",
    "# Allocate up to 95% of GPU memory for this process (prevents memory overflows)\n",
    "torch.cuda.set_per_process_memory_fraction(0.95)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Setting Multiprocessing Start Method. (Required by CUDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the \"spawn\" method for starting new processes.\n",
    "# This is recommended for PyTorch to avoid issues with shared memory.\n",
    "# BUT, doesn't work in Jupyter, so no point defining it\n",
    "\n",
    "# multiprocessing.set_start_method(\"spawn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model save path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = \"final\"  # Global model save path\n",
    "\n",
    "# Alternatively, select the best one I trained\n",
    "# MODEL_PATH = \"final_296\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Adaptive Hyperparameter Tuning for RL\n",
    "\n",
    "This callback is the **core differentiator** of the model‚Äîit introduces a **dynamic hyperparameter adjustment mechanism** that reacts to training performance in real-time. \n",
    "\n",
    "Unlike static training setups, it **modifies exploration, optimization, and learning rate on the fly**, based on episode length and rewards.\n",
    "\n",
    "### Key Features:\n",
    "- **üöÄ Adapts to training conditions**:\n",
    "  - If training **stagnates with long episodes**, it **increases exploration**.\n",
    "  - If **rewards improve**, it **fine-tunes learning stability**.\n",
    "  - If **performance drops**, it **resets to baseline** for recovery.\n",
    "\n",
    "This **adaptive tuning** allows the model to **self-optimize over time**, making learning more **efficient and responsive** to training conditions. üöÄ\n",
    "\n",
    "-------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdjustHyperparamsCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    Hyperparameters managed:\n",
    "    - `ent_coef` (Entropy coefficient): Controls exploration.\n",
    "    - `vf_coef` (Value function coefficient): Balances policy and value loss.\n",
    "    - `n_epochs` (Number of training epochs): Defines how often each batch is trained.\n",
    "    - `lr` (Learning rate): Adjusted dynamically for better convergence.\n",
    "    - `threshold` (Episode length threshold): Used to decide when to modify hyperparameters.\n",
    "\n",
    "    The updated hyperparameters are saved to a JSON file (`hyperparams.json`) to maintain consistency across training runs.\n",
    "\n",
    "    Attributes:\n",
    "        env (VecEnv): The training environment (vectorized).\n",
    "        percentage (float): Fraction of max episode steps used as threshold for modification.\n",
    "        window_size (int): Number of recent episodes to consider for statistics.\n",
    "        save_path (str): Path to save hyperparameters.\n",
    "        modified_length (bool): Tracks if episode length-based modifications were applied.\n",
    "        modified_reward (bool): Tracks if reward-based modifications were applied.\n",
    "        ent_coef (float): Entropy coefficient for exploration.\n",
    "        vf_coef (float): Value function coefficient.\n",
    "        n_epochs (int): Number of training epochs.\n",
    "        lr (float): Learning rate.\n",
    "\n",
    "    Methods:\n",
    "        save_hyperparams():\n",
    "            Saves the updated hyperparameters to a JSON file.\n",
    "\n",
    "        load_hyperparams():\n",
    "            Loads hyperparameters from a JSON file if it exists.\n",
    "\n",
    "        _on_step():\n",
    "            Evaluates recent training performance and adjusts hyperparameters dynamically.\n",
    "    \"\"\"\n",
    "    def __init__(self, env, percentage=1, window_size=100, verbose=0, save_path=\"hyperparams.json\"):\n",
    "        super().__init__(verbose)\n",
    "        self.window_size = window_size  # Moving average window\n",
    "        self.save_path = save_path\n",
    "\n",
    "        # Compute a global threshold based on the average max_episode_steps across envs\n",
    "        max_steps = np.mean([e.spec.max_episode_steps for e in env.envs])\n",
    "        self.threshold = int(max_steps * percentage)\n",
    "\n",
    "        # Global flags to track if modifications have been applied\n",
    "        self.modified_length = False\n",
    "        self.modified_reward = False\n",
    "\n",
    "        # Default global hyperparameters\n",
    "        self.ent_coef = 0.01\n",
    "        # self.gamma = 0.999\n",
    "        self.vf_coef = 0.5\n",
    "        self.n_epochs = 8\n",
    "        self.lr = 0.01\n",
    "\n",
    "        # Load previous hyperparameters if available\n",
    "        self.load_hyperparams()\n",
    "\n",
    "    def save_hyperparams(self):\n",
    "        \"\"\"Save modified hyperparameters to a JSON file.\"\"\"\n",
    "        hyperparams = {\n",
    "            \"ent_coef\": self.ent_coef,\n",
    "            # \"gamma\": self.gamma,\n",
    "            \"vf_coef\": self.vf_coef,\n",
    "            \"n_epochs\": self.n_epochs,\n",
    "            \"modified_length\": self.modified_length,\n",
    "            \"modified_reward\": self.modified_reward,\n",
    "            \"threshold\": self.threshold,\n",
    "            \"lr\": self.lr\n",
    "        }\n",
    "        with open(self.save_path, \"w\") as f:\n",
    "            json.dump(hyperparams, f)\n",
    "    \n",
    "    def load_hyperparams(self):\n",
    "        \"\"\"Load modified hyperparameters from a JSON file if it exists.\"\"\"\n",
    "        if os.path.exists(self.save_path):\n",
    "            with open(self.save_path, \"r\") as f:\n",
    "                hyperparams = json.load(f)\n",
    "                self.ent_coef = hyperparams.get(\"ent_coef\", self.ent_coef)\n",
    "                # self.gamma = hyperparams.get(\"gamma\", self.gamma)\n",
    "                self.vf_coef = hyperparams.get(\"vf_coef\", self.vf_coef)\n",
    "                self.n_epochs = hyperparams.get(\"n_epochs\", self.n_epochs)\n",
    "                self.modified_length = hyperparams.get(\"modified_length\", self.modified_length)\n",
    "                self.modified_reward = hyperparams.get(\"modified_reward\", self.modified_reward)\n",
    "                self.threshold = hyperparams.get(\"threshold\", self.threshold)\n",
    "                self.lr = hyperparams.get(\"lr\", self.lr)\n",
    "                print(\"‚úÖ Loaded previous hyperparameters from file.\")\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        \"\"\"\n",
    "        Dynamically adjusts hyperparameters based on recent training performance.\n",
    "\n",
    "        This method is executed at each training step and analyzes recent episode statistics (length and reward)\n",
    "        to modify key hyperparameters adaptively. The goal is to:\n",
    "        1. Increase exploration if episodes are too long but rewards are high.\n",
    "        2. Fine-tune learning rate and optimization parameters based on reward trends.\n",
    "        3. Reset hyperparameters if performance drops.\n",
    "\n",
    "        Hyperparameters adjusted:\n",
    "        - `ent_coef`: Entropy coefficient (controls exploration).\n",
    "        - `vf_coef`: Value function coefficient.\n",
    "        - `n_epochs`: Number of training epochs.\n",
    "        - `lr`: Learning rate.\n",
    "        - `threshold`: Adjusted based on training performance.\n",
    "\n",
    "        Returns:\n",
    "            bool: `True` to continue training, `False` to stop the current `.learn()` call if major adjustments occur.\n",
    "        \"\"\"\n",
    "\n",
    "        lr_min, lr_max = 0.0001, 0.01  # Define learning rate range\n",
    "\n",
    "        # Ensure there is at least one episode in the buffer.\n",
    "        if self.model.ep_info_buffer:\n",
    "\n",
    "            # Compute global statistics over the most recent episodes.\n",
    "            recent_eps = list(self.model.ep_info_buffer)[-self.window_size:]\n",
    "            recent_ep_lengths = [ep[\"l\"] for ep in recent_eps]\n",
    "            recent_ep_rewards = [ep[\"r\"] for ep in recent_eps]\n",
    "\n",
    "            smoothed_ep_len_mean = np.mean(recent_ep_lengths)\n",
    "            smoothed_ep_rew_mean = np.mean(recent_ep_rewards)\n",
    "\n",
    "            # ------------------------------------------------------------\n",
    "            # 1. If the episode length is too high and rewards are high, \n",
    "            #    increase exploration and adjust learning rate.\n",
    "            # ------------------------------------------------------------\n",
    "            if smoothed_ep_len_mean >= self.threshold and smoothed_ep_rew_mean > 120 and not self.modified_length:\n",
    "                self.ent_coef = 0.015  # Increase entropy to encourage exploration\n",
    "                # self.gamma = 0.99  # Focus on short-term rewards\n",
    "                \n",
    "                # Adjust learning rate based on reward scaling\n",
    "                self.lr = lr_min + (lr_max - lr_min) * np.log1p((315 - smoothed_ep_rew_mean) / 315) / np.log1p(1)\n",
    "\n",
    "                self.modified_length = True  # Mark that modifications were applied\n",
    "\n",
    "                if self.verbose > 0:\n",
    "                    print(f\"üîÑ High episode length (mean: {smoothed_ep_len_mean:.2f}) detected.\")\n",
    "                    print(f\"    Updating hyperparams: ent_coef={self.ent_coef}, gamma={self.gamma}\")\n",
    "\n",
    "                self.save_hyperparams()\n",
    "                return False  # Stop training for adjustments to take effect\n",
    "\n",
    "            # ------------------------------------------------------------\n",
    "            # 2. If episode length is below the threshold and adjustments \n",
    "            #    were previously made, fine-tune based on reward trends.\n",
    "            # ------------------------------------------------------------\n",
    "            elif smoothed_ep_len_mean < self.threshold and self.modified_length:\n",
    "                self.lr = lr_min + (lr_max - lr_min) * np.log1p((315 - smoothed_ep_rew_mean) / 315) / np.log1p(1)\n",
    "\n",
    "                # If rewards are high, refine hyperparameters to improve performance\n",
    "                if smoothed_ep_rew_mean >= 200:\n",
    "                    min_ent_coef, max_ent_coef = 0.001, 0.01\n",
    "                    # min_gamma, max_gamma = 0.999, 0.9999\n",
    "                    min_vf_coef, max_vf_coef = 0.5, 1.0\n",
    "                    min_n_epochs, max_n_epochs = 8, 28\n",
    "\n",
    "                    # Normalize reward between 0 and 1\n",
    "                    normalized_reward = max(0, min((smoothed_ep_rew_mean - 200) / (315 - 200), 1))\n",
    "                    self.ent_coef = max_ent_coef - (max_ent_coef - min_ent_coef) * normalized_reward\n",
    "                    # self.gamma = min_gamma + (max_gamma - min_gamma) * normalized_reward\n",
    "                    self.vf_coef = min_vf_coef + (max_vf_coef - min_vf_coef) * normalized_reward\n",
    "                    self.n_epochs = int(min_n_epochs + (max_n_epochs - min_n_epochs) * normalized_reward)\n",
    "\n",
    "                    # Gradually decrease the threshold\n",
    "                    self.threshold = max(self.threshold - int(3 * normalized_reward), 130)\n",
    "\n",
    "                    self.modified_reward = True  # Mark that reward-based modifications were applied\n",
    "\n",
    "                    if self.verbose > 0:\n",
    "                        print(f\"üîÑ Adjusted hyperparams based on high reward (mean: {smoothed_ep_rew_mean:.2f})\")\n",
    "                        # print(f\"    ent_coef: {self.ent_coef:.5f}, gamma: {self.gamma:.6f}, vf_coef: {self.vf_coef:.2f}\")\n",
    "\n",
    "                # ------------------------------------------------------------\n",
    "                # 3. If rewards drop significantly, reset hyperparameters.\n",
    "                # ------------------------------------------------------------\n",
    "                elif smoothed_ep_rew_mean < 175 and self.modified_reward:\n",
    "                    self.ent_coef = 0.01  # Reset entropy coefficient\n",
    "                    # self.gamma = 0.999  # Reset gamma\n",
    "                    self.vf_coef = 0.5  # Reset value function coefficient\n",
    "                    self.n_epochs = 8  # Reset epochs to default\n",
    "                    self.modified_reward = False  # Reset flag\n",
    "\n",
    "                    if self.verbose > 0:\n",
    "                        print(\"üîÑ Reset hyperparameters due to reward drop.\")\n",
    "\n",
    "            # ------------------------------------------------------------\n",
    "            # Apply the final hyperparameter changes to the model.\n",
    "            # ------------------------------------------------------------\n",
    "            self.model.ent_coef = self.ent_coef\n",
    "            # self.model.gamma = self.gamma\n",
    "            self.model.vf_coef = self.vf_coef\n",
    "            self.model.n_epochs = self.n_epochs\n",
    "\n",
    "            # Save updated hyperparameters to JSON\n",
    "            self.save_hyperparams()\n",
    "\n",
    "        return True  # Continue training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation Function\n",
    "\n",
    "Next, we define the `evaluate` function, which will **run the evaluation process** for a trained reinforcement learning model in the **LunarLander-v3** environment. \n",
    "\n",
    "This function serves three main purposes:\n",
    "\n",
    "1. **Scoring the Model**  \n",
    "   - It runs multiple episodes and calculates the **total reward** obtained by the agent in each episode.  \n",
    "   - It also records the **number of steps** the agent takes before reaching a terminal state (either success or failure).  \n",
    "   - These metrics help assess how well the model performs in terms of both efficiency and effectiveness.\n",
    "\n",
    "2. **Providing a Visualization**  \n",
    "   - The environment is launched with `render_mode=\"human\"`, allowing real-time visualization of the agent‚Äôs actions.  \n",
    "   - The function uses `env.render()` at each step to display the agent's interactions with the environment.  \n",
    "   - A **small delay (`time.sleep(0.01)`)** is introduced to make the animation smoother and easier to follow.\n",
    "\n",
    "3. **Ensuring Controlled Execution**  \n",
    "   - The model is loaded from a specified path and evaluated using deterministic policy execution (`deterministic=True`), ensuring repeatability.  \n",
    "   - The function limits the number of steps per episode to **250** to prevent showing landing episodes that last too long, potentially causing multiple eval visualizations to run at the same time.  \n",
    "   - After evaluation, the environment is properly closed using `env.close()` to free system resources.\n",
    "\n",
    "This evaluation step is crucial for diagnosing the model‚Äôs performance and identifying areas for improvement, such as hyperparameter tuning or additional training.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model_path, episodes=3):\n",
    "    \"\"\"\n",
    "    Evaluates a trained PPO model on the LunarLander-v3 environment.\n",
    "\n",
    "    This function loads a pretrained model, runs it in the environment for a specified \n",
    "    number of episodes, and renders the gameplay while tracking the total reward and steps.\n",
    "\n",
    "    Features:\n",
    "    - Uses `render_mode=\"human\"` to visualize the agent's behavior.\n",
    "    - Loads the model from the specified `model_path` and selects GPU (`cuda`) if available.\n",
    "    - Runs multiple episodes, tracking rewards and number of steps per episode.\n",
    "    - Enforces a maximum step limit of 250 per episode for stability.\n",
    "    - Introduces a small time delay (`0.01s`) between steps to make visualization smoother.\n",
    "\n",
    "    Args:\n",
    "        model_path (str): Path to the saved PPO model file.\n",
    "        episodes (int): Number of episodes to evaluate. Default is 3.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create the environment with rendering enabled for visualization.\n",
    "    env = gym.make(\"LunarLander-v3\", render_mode=\"human\")\n",
    "\n",
    "    # Load the trained PPO model with the environment attached.\n",
    "    model = PPO.load(model_path, env=env, device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    for ep in range(episodes):\n",
    "        # Reset the environment for a new episode.\n",
    "        obs, info = env.reset()\n",
    "        done = False\n",
    "        truncated = False\n",
    "        ep_reward = 0  # Track total episode reward\n",
    "        steps = 0  # Track number of steps in the episode\n",
    "\n",
    "        while not (done or truncated):\n",
    "            env.render()  # Render the environment for visualization\n",
    "            \n",
    "            # Select action from the trained model (deterministic mode for evaluation)\n",
    "            action, _ = model.predict(obs, deterministic=True)\n",
    "            \n",
    "            # Execute the action in the environment and receive the next state and reward.\n",
    "            obs, reward, done, truncated, info = env.step(action)\n",
    "            \n",
    "            ep_reward += reward  # Accumulate total reward\n",
    "            steps += 1  # Count steps\n",
    "\n",
    "            # Enforce a maximum step limit per episode to prevent infinite loops\n",
    "            if steps == 250:\n",
    "                done = True\n",
    "\n",
    "            time.sleep(0.01)  # Small delay for smoother rendering\n",
    "\n",
    "        print(f\"Episode {ep+1} reward: {ep_reward:.4f}, Steps= {steps}\")\n",
    "\n",
    "    env.close()  # Close the environment to free resources\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Training PPO on LunarLander-v3 with Adaptive Hyperparameters\n",
    "\n",
    "This function trains a **Proximal Policy Optimization (PPO) agent** in the **LunarLander-v3** environment, leveraging **dynamic hyperparameter tuning** to optimize performance over time.\n",
    "\n",
    "### üîπ Key Features:\n",
    "- **Automatic Hyperparameter Adjustment**  \n",
    "  - Uses `AdjustHyperparamsCallback` to modify **exploration**, **learning rate**, and other key parameters in response to training conditions.\n",
    "  \n",
    "- **Parallel Training for Faster Convergence**  \n",
    "  - Runs **64 parallel environments** to accelerate learning.\n",
    "\n",
    "- **Model Persistence & Periodic Checkpoints**  \n",
    "  - Automatically **loads an existing model** if available, allowing incremental training.\n",
    "  - Periodically **saves the model** and **evaluates it in a separate process**.\n",
    "\n",
    "- **Restart Mechanism for Adaptive Learning**  \n",
    "  - If performance stagnates or improves significantly, the PPO model **restarts with new hyperparameters**.\n",
    "\n",
    "### üìå Training Details:\n",
    "- **Policy Architecture:** 2-layer MLP with sizes `[128, 64]`\n",
    "- **Optimizer:** `AdamW` with weight decay (`1e-2`)\n",
    "- **Training Steps:** `user defined`\n",
    "- **Evaluation Frequency:** Every `500,000` steps\n",
    "\n",
    "This implementation ensures that the agent **adapts dynamically**, making it more **robust and efficient** compared to static training approaches. üöÄ\n",
    "\n",
    "\n",
    "--------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_ppo(total_timesteps):\n",
    "    \"\"\"\n",
    "    Train a PPO (Proximal Policy Optimization) agent on the LunarLander-v3 environment.\n",
    "\n",
    "    This function initializes or loads a PPO model and continuously trains it while adjusting \n",
    "    hyperparameters dynamically. The model is periodically restarted when the `AdjustHyperparamsCallback` \n",
    "    signals that adjustments are necessary.\n",
    "\n",
    "    Key Features:\n",
    "    - Uses **Stable-Baselines3 PPO** with a custom neural network architecture.\n",
    "    - Supports **dynamic hyperparameter tuning** via the `AdjustHyperparamsCallback`.\n",
    "    - Implements **periodic model saving** and evaluation in a separate process.\n",
    "    - Trains on **multiple environments in parallel (64 environments)** for faster learning.\n",
    "    - Uses **adaptive exploration strategies** to optimize agent performance.\n",
    "\n",
    "    Args:\n",
    "        total_timesteps (int): The total number of training timesteps before stopping.\n",
    "    \"\"\"\n",
    "\n",
    "    # Number of parallel environments to use for training\n",
    "    n_envs = 64  \n",
    "\n",
    "    # Flag to track if reward-based modifications have been applied\n",
    "    rew_mod_anytime = False  \n",
    "\n",
    "    # Define the policy architecture and optimizer settings\n",
    "    policy_kwargs = dict(\n",
    "        optimizer_class=AdamW,  # Use AdamW optimizer for better weight decay handling\n",
    "        optimizer_kwargs=dict(weight_decay=1e-2),  # Weight decay for regularization\n",
    "        net_arch=dict(pi=[128, 64], vf=[128, 64])  # Define network structure for policy and value functions\n",
    "    )\n",
    "\n",
    "    # Create a vectorized environment with a fixed episode length of 250 steps\n",
    "    env = make_vec_env(\"LunarLander-v3\", n_envs=n_envs, env_kwargs={\"max_episode_steps\": 250})\n",
    "\n",
    "    # Set a fixed random seed for reproducibility\n",
    "    env.seed(SEED)\n",
    "\n",
    "    # Select the computation device: Use GPU if available, otherwise fall back to CPU\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    # -----------------------------------------------\n",
    "    # Load an existing model if available; otherwise, train from scratch\n",
    "    # -----------------------------------------------\n",
    "    if os.path.exists(MODEL_PATH + \".zip\"):\n",
    "        print(f\"Loading existing model from {MODEL_PATH}...\")\n",
    "\n",
    "        # Load hyperparameters from a JSON file (if previously saved)\n",
    "        with open(\"hyperparams.json\", \"r\") as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        lr_value = data.get(\"lr\")  # Extract the stored learning rate\n",
    "\n",
    "        # Load the pretrained PPO model with the environment and existing hyperparameters\n",
    "        model = PPO.load(\n",
    "            MODEL_PATH,\n",
    "            env=env,\n",
    "            device=device,\n",
    "            learning_rate=lr_value,  # Attempt to apply the saved learning rate\n",
    "            custom_objects=data,  # Load other hyperparameters dynamically\n",
    "            policy_kwargs=policy_kwargs\n",
    "        )\n",
    "\n",
    "        # Manually update the optimizer's learning rate (alternative approach)\n",
    "        for param_group in model.policy.optimizer.param_groups:\n",
    "            param_group[\"lr\"] = lr_value\n",
    "\n",
    "        # Get the current number of timesteps already trained\n",
    "        current_timesteps = model.num_timesteps\n",
    "\n",
    "    else:\n",
    "        print(\"No existing model found. Training from scratch...\")\n",
    "\n",
    "        # Initialize a new PPO model with predefined hyperparameters\n",
    "        model = PPO(\n",
    "            policy=\"MlpPolicy\",  # Use a Multi-Layer Perceptron (MLP) policy\n",
    "            env=env,\n",
    "            learning_rate=lambda _: 0.01,  # Initial learning rate (adjustable later)\n",
    "            n_steps=512,  # Number of steps per environment before updating\n",
    "            batch_size=4096,  # Batch size for training updates\n",
    "            n_epochs=8,  # Number of passes over each batch during training\n",
    "            gamma=0.999,  # Discount factor for future rewards\n",
    "            gae_lambda=0.98,  # Generalized Advantage Estimation lambda\n",
    "            clip_range=lambda _: 0.2,  # Clipping range for PPO objective\n",
    "            ent_coef=0.01,  # Entropy coefficient (controls exploration)\n",
    "            vf_coef=0.5,  # Value function coefficient (importance of critic loss)\n",
    "            max_grad_norm=0.5,  # Gradient clipping for stability\n",
    "            use_sde=False,  # Whether to use State-Dependent Exploration (SDE)\n",
    "            normalize_advantage=True,  # Normalize advantages for stability\n",
    "            verbose=1,  # Print training logs\n",
    "            device=device,  # Set computation device (GPU/CPU)\n",
    "            tensorboard_log=\"./final_tensorboard\",  # Path for TensorBoard logs\n",
    "            policy_kwargs=policy_kwargs  # Apply custom network architecture\n",
    "        )\n",
    "\n",
    "        # Since training starts fresh, initialize current timestep count to 0\n",
    "        current_timesteps = 0\n",
    "\n",
    "    # -----------------------------------------------\n",
    "    # Initialize the hyperparameter tuning callback\n",
    "    # -----------------------------------------------\n",
    "    callback = AdjustHyperparamsCallback(\n",
    "        env, percentage=0.999, window_size=n_envs * 512 * 2, verbose=0  # You can turn verbosity on but it will cause a lot of outputs\n",
    "    )\n",
    "\n",
    "    # -----------------------------------------------\n",
    "    # Main training loop with periodic evaluation\n",
    "    # -----------------------------------------------\n",
    "    eval_interval = 500_000  # Evaluate the model every 500,000 steps\n",
    "\n",
    "    while current_timesteps < total_timesteps:\n",
    "        # Determine how many steps to train in this cycle\n",
    "        train_step = min(eval_interval, total_timesteps - current_timesteps)\n",
    "\n",
    "        # Retrieve updated hyperparameters from the callback\n",
    "        new_hyperparams = {\n",
    "            \"ent_coef\": callback.ent_coef,\n",
    "            # \"gamma\": callback.gamma,\n",
    "            \"vf_coef\": callback.vf_coef,\n",
    "            \"n_epochs\": callback.n_epochs,\n",
    "            \"threshold\": callback.threshold,\n",
    "            \"modified_length\": callback.modified_length,\n",
    "            \"modified_reward\": callback.modified_reward,\n",
    "            \"lr\": callback.lr,      \n",
    "        }\n",
    "\n",
    "        # Restart training with updated hyperparameters if needed\n",
    "        if os.path.exists(MODEL_PATH + \".zip\"):\n",
    "            # Track whether rewards have been modified anytime during training\n",
    "            rew_mod_anytime = rew_mod_anytime or callback.modified_reward\n",
    "\n",
    "            print(f\"Restarting PPO with new hyperparameters: {new_hyperparams}\")\n",
    "\n",
    "            if rew_mod_anytime:\n",
    "                # Reload the model with the new hyperparameters if reward-based modifications were applied\n",
    "                model = PPO.load(\n",
    "                    MODEL_PATH,\n",
    "                    env=env,\n",
    "                    device=device,\n",
    "                    learning_rate=callback.lr,  # Apply the new learning rate\n",
    "                    custom_objects=new_hyperparams,  # Load updated hyperparams\n",
    "                    policy_kwargs=policy_kwargs\n",
    "                )\n",
    "                for param_group in model.policy.optimizer.param_groups:\n",
    "                    param_group[\"lr\"] = callback.lr\n",
    "            else:\n",
    "                # Reload the model with non-reward-modified hyperparameters\n",
    "                model = PPO.load(\n",
    "                    MODEL_PATH,\n",
    "                    env=env,\n",
    "                    device=device,\n",
    "                    custom_objects=new_hyperparams,\n",
    "                    policy_kwargs=policy_kwargs\n",
    "                )\n",
    "\n",
    "        # -----------------------------------------------\n",
    "        # Train the model for the current step interval\n",
    "        # -----------------------------------------------\n",
    "        model.learn(total_timesteps=train_step, reset_num_timesteps=False, callback=callback)\n",
    "        current_timesteps += train_step  # Update the total timesteps count\n",
    "\n",
    "        # -----------------------------------------------\n",
    "        # Save the model after training iteration\n",
    "        # -----------------------------------------------\n",
    "        model._logger = None  # Remove logger to avoid pickling issues\n",
    "        model.save(MODEL_PATH)  # Save the trained model\n",
    "\n",
    "        # -----------------------------------------------\n",
    "        # Launch evaluation in a separate process\n",
    "        # This doesn't work in the jupyter environment so we comment it out here\n",
    "        # -----------------------------------------------\n",
    "        # eval_process = multiprocessing.Process(target=evaluate, args=(MODEL_PATH,))\n",
    "        # eval_process.start()  # Run evaluation asynchronously\n",
    "\n",
    "    # Close the environment after training is complete\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Running PPO Training\n",
    "\n",
    "Now that we have defined our **adaptive PPO training function**, it's time to **start training** the agent on **LunarLander-v3**.\n",
    "\n",
    "### üî• Training Execution:\n",
    "- The model will train for **20,000,000 timesteps**.\n",
    "- If an **existing model** is found, training will **resume** from the last checkpoint.\n",
    "- Adaptive hyperparameter tuning is enabled via `AdjustHyperparamsCallback`.\n",
    "- The model will be periodically **evaluated and saved** to prevent loss of progress.\n",
    "\n",
    "‚è≥ Expected Duration:\n",
    "Training for 20M timesteps may take a significant amount of time depending on:\n",
    "\n",
    "- GPU availability (CUDA acceleration).\n",
    "- Number of CPU cores used for parallel environments.\n",
    "- Training efficiency (batch sizes, optimization settings).\n",
    "\n",
    "\n",
    "Monitor training logs for progress and periodic evaluations! üöÄ\n",
    "\n",
    "-------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with PPO...\n",
      "Loading existing model from final...\n",
      "‚úÖ Loaded previous hyperparameters from file.\n"
     ]
    }
   ],
   "source": [
    "# Actual training\n",
    "print(\"Training with PPO...\")\n",
    "train_with_ppo(total_timesteps=10_000_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If you just want the evaluation visualization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Episode 1 reward: 294.8767, Steps= 153\n",
      "Episode 2 reward: 287.6180, Steps= 180\n",
      "Episode 3 reward: 314.0656, Steps= 192\n",
      "Episode 4 reward: 314.5264, Steps= 200\n",
      "Episode 5 reward: 301.8995, Steps= 179\n"
     ]
    }
   ],
   "source": [
    "evaluate(MODEL_PATH, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the result to a gif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ruben/miniconda3/envs/rapids-23.10/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py:150: UserWarning: You are trying to run PPO on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ GIF saved to lunar_lander.gif\n"
     ]
    }
   ],
   "source": [
    "from eval_only import record_lunar_lander_gif\n",
    "\n",
    "record_lunar_lander_gif(MODEL_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids-23.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
