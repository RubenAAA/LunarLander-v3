# ğŸš€ Reinforcement Learning with PPO on LunarLander-v3

## ğŸ“ Project Overview
This project implements **Proximal Policy Optimization (PPO)** to train an agent in the **LunarLander-v3** environment using **Stable-Baselines3**. The training process leverages **dynamic hyperparameter tuning**, parallel environments, and periodic model evaluations to optimize performance.

## ğŸ¯ Features
- **Custom Adaptive Hyperparameter Tuning**  
  - Adjusts **entropy coefficient**, **learning rate**, and **training epochs** in response to training conditions.
  - Uses a **custom callback** (`AdjustHyperparamsCallback`) to modify hyperparameters on the fly.

- **Efficient Parallel Training**  
  - Runs **64 environments in parallel** to speed up learning.
  - Optimized for **CUDA** acceleration with `torch`.

- **Periodic Model Checkpointing & Evaluation**  
  - Saves the trained model at intervals.
  - Evaluates performance in a separate **multiprocessing** process.

- **Multiprocessing Support in Jupyter Notebooks**  
  - Works with `"spawn"` to avoid CUDA multiprocessing issues.
  - Ensures compatibility with notebooks and Python scripts.

## ğŸ› ï¸ Installation
First, clone the repository:
```bash
git clone https://github.com/your-username/your-repo.git
cd your-repo
```
## ğŸ“Œ Install Dependencies
To install all required dependencies, run:
```bash
pip install -r requirements.txt
```

## ğŸš€ Running the Training
### ğŸ“Œ Training from Scratch:
```bash
python
from train import train_with_ppo

train_with_ppo(total_timesteps=20_000_000)
```

### ğŸ“Œ Running Evaluation:
```bash
python
from evaluate import evaluate

evaluate("trained_model.zip")
```

## ğŸ–¥ï¸ Running in Jupyter Notebook
If using Jupyter Notebook, ensure **multiprocessing compatibility** by setting:
```bash
python
import multiprocessing
multiprocessing.set_start_method("spawn", force=True)
```
Then, restart the kernel before running the training.

## ğŸ“‚ Project Structure
```bash
ğŸ“¦ lunar_lander/
 â”£ ğŸ“œ __pycache__/         # Compiled Python files
 â”£ ğŸ“œ final_tensorboard/   # TensorBoard logs
 â”£ ğŸ“œ .gitattributes       # Git configuration file
 â”£ ğŸ“œ eval_only.py         # Script for evaluation-only purposes
 â”£ ğŸ“œ final_10m_timesteps_292.zip  # Model checkpoint file
 â”£ ğŸ“œ final_285.zip        # Model checkpoint file
 â”£ ğŸ“œ final_285.py         # Script associated with final_285.zip
 â”£ ğŸ“œ final_291.zip        # Model checkpoint file
 â”£ ğŸ“œ final_293.zip        # Model checkpoint file
 â”£ ğŸ“œ final_296.zip        # Model checkpoint file
 â”£ ğŸ“œ final.py             # Main training script
 â”£ ğŸ“œ final.zip            # Final model checkpoint
 â”£ ğŸ“œ hyperparams.json     # JSON file for hyperparameters
 â”£ ğŸ“œ pipeline.ipynb       # Jupyter Notebook for pipeline
 â”£ ğŸ“œ rapids-23.10.yml     # Environment configuration for RAPIDS
 â”£ ğŸ“œ README.md            # Project documentation
 â”£ ğŸ“œ requirements.txt     # Required dependencies for the project
```

## ğŸ“Œ Hyperparameters Used
| Parameter     | Value    |
|--------------|---------|
| `policy`     | MlpPolicy |
| `n_steps`    | 512     |
| `batch_size` | 4096    |
| `n_epochs`   | 8       |
| `gamma`      | 0.999   |
| `gae_lambda` | 0.98    |
| `clip_range` | 0.2     |
| `ent_coef`   | 0.01    |
| `vf_coef`    | 0.5     |

## ğŸ“œ License
This project is **open-source** and available under the MIT License.

---

ğŸ”¥ **Feel free to contribute, report issues, or suggest improvements!** ğŸ”¥
